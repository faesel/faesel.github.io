1:"$Sreact.fragment"
2:I[31362,["/_next/static/chunks/8be72f203fe36d6f.js"],"default"]
3:I[2971,["/_next/static/chunks/8be72f203fe36d6f.js"],"default"]
4:I[39756,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/708205fa81a1891d.js"],"default"]
5:I[37457,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/708205fa81a1891d.js"],"default"]
6:I[22016,["/_next/static/chunks/8be72f203fe36d6f.js"],""]
8:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/708205fa81a1891d.js"],"OutletBoundary"]
9:"$Sreact.suspense"
b:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/708205fa81a1891d.js"],"ViewportBoundary"]
d:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/708205fa81a1891d.js"],"MetadataBoundary"]
f:I[68027,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/708205fa81a1891d.js"],"default"]
:HL["/_next/static/chunks/19d110f125a6a880.css","style"]
:HL["/_next/static/chunks/1d99135141f200e4.css","style"]
:HL["/_next/static/chunks/578ca6ed020117e9.css","style"]
0:{"P":null,"b":"GRQ4Wn3Dp8FngowyVAxrt","c":["","blog"],"q":"","i":false,"f":[[["",{"children":["blog",{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/19d110f125a6a880.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","script","script-0",{"src":"/_next/static/chunks/8be72f203fe36d6f.js","async":true,"nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":[["$","head",null,{"children":[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"Organization\",\"name\":\"Faesel Saeed\",\"url\":\"https://www.faesel.com\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https://www.faesel.com/images/logo.png\"},\"sameAs\":[\"https://github.com/faeselsaeed\",\"https://www.linkedin.com/in/faesel-saeed-a97b1614\"]}"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"name\":\"FAESEL.COM\",\"url\":\"https://www.faesel.com\",\"description\":\"A modern tech blog exploring technology, coding, and digital innovation\",\"publisher\":{\"@type\":\"Organization\",\"name\":\"Faesel Saeed\",\"url\":\"https://www.faesel.com\"}}"}}]]}],["$","body",null,{"children":[["$","$L2",null,{"gaId":"test"}],["$","a",null,{"href":"#main-content","className":"skip-to-content","children":"Skip to content"}],["$","$L3",null,{}],["$","main",null,{"id":"main-content","style":{"minHeight":"70vh"},"children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","div",null,{"className":"not-found-module__HS70Aa__container","children":[["$","h1",null,{"className":"not-found-module__HS70Aa__title","children":"404"}],["$","h2",null,{"className":"not-found-module__HS70Aa__subtitle","children":"Page Not Found"}],["$","p",null,{"className":"not-found-module__HS70Aa__message","children":"Sorry, the page you're looking for doesn't exist."}],["$","$L6",null,{"href":"/","className":"not-found-module__HS70Aa__homeLink","children":"Go Home"}]]}],[["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/578ca6ed020117e9.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","footer",null,{"className":"Footer-module__S6Hkya__footer","children":["$","div",null,{"className":"Footer-module__S6Hkya__container","children":["$","div",null,{"className":"Footer-module__S6Hkya__content","children":[["$","nav",null,{"className":"Footer-module__S6Hkya__links","aria-label":"Footer navigation","children":[["$","$L6",null,{"href":"/blog","className":"Footer-module__S6Hkya__link","children":"Blog"}],["$","$L6",null,{"href":"/projects","className":"Footer-module__S6Hkya__link","children":"Projects"}],["$","$L6",null,{"href":"/about","className":"Footer-module__S6Hkya__link","children":"About"}],["$","$L6",null,{"href":"/contact","className":"Footer-module__S6Hkya__link","children":"Contact"}]]}],["$","p",null,{"className":"Footer-module__S6Hkya__copyright","children":["Â© ",2026," All rights reserved."]}]]}]}]}]]}]]}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":["$L7",[["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/1d99135141f200e4.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","script","script-0",{"src":"/_next/static/chunks/d2d257db68345a02.js","async":true,"nonce":"$undefined"}]],["$","$L8",null,{"children":["$","$9",null,{"name":"Next.MetadataOutlet","children":"$@a"}]}]]}],{},null,false,false]},null,false,false]},null,false,false],["$","$1","h",{"children":[null,["$","$Lb",null,{"children":"$Lc"}],["$","div",null,{"hidden":true,"children":["$","$Ld",null,{"children":["$","$9",null,{"name":"Next.Metadata","children":"$Le"}]}]}],null]}],false]],"m":"$undefined","G":["$f",[]],"S":true}
c:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
10:I[27201,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/708205fa81a1891d.js"],"IconMark"]
a:null
e:[["$","title","0",{"children":"Blog | Tech Blog | FAESEL.COM"}],["$","meta","1",{"name":"description","content":"Read the latest articles about technology, coding, and digital innovation"}],["$","link","2",{"rel":"author","href":"https://www.faesel.com/about"}],["$","meta","3",{"name":"author","content":"Faesel Saeed"}],["$","meta","4",{"name":"keywords","content":"blog,technology,web development,programming,software engineering"}],["$","meta","5",{"property":"og:title","content":"Blog | Tech Blog"}],["$","meta","6",{"property":"og:description","content":"Read the latest articles about technology, coding, and digital innovation"}],["$","meta","7",{"property":"og:type","content":"website"}],["$","meta","8",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","9",{"name":"twitter:site","content":"@faeselsaeed"}],["$","meta","10",{"name":"twitter:creator","content":"@faeselsaeed"}],["$","meta","11",{"name":"twitter:title","content":"Blog | Tech Blog"}],["$","meta","12",{"name":"twitter:description","content":"Read the latest articles about technology, coding, and digital innovation"}],["$","link","13",{"rel":"shortcut icon","href":"/favicon.ico"}],["$","link","14",{"rel":"icon","href":"/favicon.ico"}],["$","link","15",{"rel":"apple-touch-icon","href":"/favicon.ico"}],["$","$L10","16",{}]]
11:I[29971,["/_next/static/chunks/8be72f203fe36d6f.js","/_next/static/chunks/d2d257db68345a02.js"],"default"]
12:T2bcd,
# Introduction

Recently I encountered a scenario where I needed to integrate New Relic into my Electron application. New Relic supports a number of integration types our the box, some of the more heavily used ones are,

- APM Agents - Primary used for backend tracking, most of the documentation will point to integrating in this style.
- Browser Agent - Used for front end client side logging.

Integrating the browser agent was a simple task of adding a logging script to the HTML page hosting the app ... easy so far. Because Electron is effectively loading up a chromium browser, the browser agent should work as normal. The APM Agent on the other hand was a different story, after having scoured the internet I found that its currently not supported with Electron. In fact if you look at the documentation for Electron, there is a suspicious absence of logging documentation. This might be down to the unique way Electrons IPC channels (inter-process-communication, allows for backend and front end communication using an event based model) work.

Due to this I knew we would require a manual approach to logging, this is where Open Telemetry comes into play. Using Open Telemetry we can bootstrap the app on startup and start manually adding tracing logs in any backend IPC handler. Since most logging providers now support Open Telemetry, including New Relic we have a mechanism to export out logs out.

This article was created as a complete guide to the approach mentioned above. Due to the lack of examples and documentation online I hope this article comes in use for someone. In the example below I was using Electron with Vite and Typescript.

# Installing Dependencies

To get started open telemetry has a couple of packages that need installing,

```bash
npm install @opentelemetry/api
npm install @opentelemetry/auto-instrumentations-node
npm install @opentelemetry/exporter-trace-otlp-http
npm install @opentelemetry/instrumentation
npm install @opentelemetry/resources
npm install @opentelemetry/semantic-conventions
```

# Bootstrapping the App

The first step to integration is to connect into the startup process of Electron and instrument our Open Telemetry tracer. As Open Telemetry allows you to export your logs to multiple 3rd parties the example below will first show you how to get started with Jaeger an open source tracing sink and then expand this to New Relic.

## Hooking into Startup

The first part of coding is to hook into the startup process your application. Electron conveniently has a `whenReady()` function that is called when electron has finished initialising. We can make use of this function to register our tracer.

Let's first start by creating a `tracing.ts` file in the root of the application. In this file we will have two functions, one to register a tracer and another to get the tracer. We are also going to pass through some useful information to the tracer like the application version, and operating system so that all our logs have some base information they can relay back to New Relic.

```javascript
export const registerTracer = (appVersion: string, operatingSystem: string): void => {
	//Registration code here
}

export const getTracer = (): Tracer => {
	//Code to get tracer here
}
```

Next we can call the register function on startup,

```javascript
import { app } from "electron"

app.whenReady().then(() => {
	//For me this is the version defined in my package.json
	const appVersion = app.getVersion();
	//Part of NodeJS's way of detirmining the platform
	const operatingSystem = process.platform;

	registerTracer(appVersion, operatingSystem);
});
```

## Configuring the Tracer

Now that we have some functions to hook into we can flesh them out with our configuration. For ease of copy and paste im going to first code dump and the file then explain each part.

```javascript
/*tracing.ts*/
import { BatchSpanProcessor } from "@opentelemetry/sdk-trace-base";
import { Resource } from "@opentelemetry/resources";
import { SemanticResourceAttributes } from "@opentelemetry/semantic-conventions";
import {
  NodeTracerProvider,
  SimpleSpanProcessor,
} from "@opentelemetry/sdk-trace-node";
import { registerInstrumentations } from "@opentelemetry/instrumentation";
import { OTLPTraceExporter } from "@opentelemetry/exporter-trace-otlp-http";

import opentelemetry, { Tracer } from "@opentelemetry/api";
import { getNodeAutoInstrumentations } from "@opentelemetry/auto-instrumentations-node";

const NEWRELIC_APP_NAME = "APP NAME GOES HERE";
const IS_DEVELOPMENT = import.meta.env.DEV;

export const registerTracer = (appVersion: string, operatingSystem: string): void => {
  registerInstrumentations({
    instrumentations: [
      getNodeAutoInstrumentations(),
      //TODO: Add an instrumentation library here when electron is supported
    ],
  });

  const resource = Resource.default().merge(
    new Resource({
      [SemanticResourceAttributes.SERVICE_NAME]: NEWRELIC_APP_NAME,
      [SemanticResourceAttributes.SERVICE_VERSION]: appVersion,
      [SemanticResourceAttributes.OS_NAME]: operatingSystem,
    })
  );

  const provider = new NodeTracerProvider({
    resource: resource,
  });

  const otlpExporter = new OTLPTraceExporter({
	//Jaeger tracing url
	url: 'http://localhost:4318/v1/traces'
  });

  const processor = IS_DEVELOPMENT
    ? new SimpleSpanProcessor(otlpExporter)
    : new BatchSpanProcessor(otlpExporter);

  provider.addSpanProcessor(processor);
  provider.register();
};
```

*Note `import.meta.env` is just Vite's way of getting hold of environment variables.*

You might firstly notice that there is a `TODO` in the code, if and when Electron supports a plugin style integration with open telemetry, the `instrumentations` array will be the place to add it. For now we are manually instrumenting the app.

The resources section is given to use using the `@opentelemetry/resources` and `@opentelemetry/semantic-conventions` packages. It allows us to define some base properties we want to show in all our tracing logs. Obvious things like the app version, and operating systems (especially if your targeting multiple systems) are really useful bits of information to add. There are also some built in attributes for Kubernetes and oddly enough AWS (with some generic ones for other cloud providers).

In terms of the `OTLPTraceExporter` block, as we opted to do our tracing using HTTP (GRPC is also an option) with the package `@opentelemetry/exporter-trace-otlp-http` i have added some basic settings for Jaeger. Note that you can add a number of exporters here, and we will also be extending this to include New Relic.

The final part in question is the processors themselves, open telemetry has a few options on how you might want to push your logs to 3rd parties. Ive opted to use a `SimpleSpanProcessor` when running the app locally so that my logs are immediately pushed to the servers when testing things out. However in production im making use of a `BatchSpanProcessor` that batches a group of logs and then pushes them in one go. This helps reduce network traffic of the app.

## Exposing a Tracer

In order to get an actual tracer we can get an instance from the global tracing provider using the `getTracer()` function. This tracer object then allows us to create logs in the shape of a `snap`. It is advised to call `getTracer` every time you need to start logging as opposed to maintaining your own instance.

```javascript
export const GetTracer = () : Tracer => {
  return opentelemetry.trace.getTracer(NEWRELIC_APP_NAME);
}
```

# Adding a Tracing Example

Now that we have added all our logging code we can finally create an actual log entry. To do this we can create a new span, in which we can raise new events and record exceptions. In the example below i've added some logging to an IPC handler.

```javascript
import { ipcMain } from 'electron'
import { GetTracer } from './tracing'

const createWindow = () => {
	//Create new browser window
	ipcMain.handle("getDinosaurs", async () => {
		await GetTracer().startActiveSpan("getDinosaurs", async (span) => {
			//await getDinosaurs()

			//Record one or many events in your code blocks
			span.addEvent("receivedDinosaur", { name: dinoName  });
			//Record exceptions in your code block
			span.recordException(new Error(`Ops their all extinct!`));
			span.end();
		});
	});
}
```

# Testing with Jaeger

To test this out with Jaeger you can first execute the following docker command to create a new instance of Jaeger with OTLP enabled.

```bash
docker run -d --name jaeger \
  -e COLLECTOR_ZIPKIN_HOST_PORT=:9411 \
  -e COLLECTOR_OTLP_ENABLED=true \
  -p 6831:6831/udp \
  -p 6832:6832/udp \
  -p 5778:5778 \
  -p 16686:16686 \
  -p 4317:4317 \
  -p 4318:4318 \
  -p 14250:14250 \
  -p 14268:14268 \
  -p 14269:14269 \
  -p 9411:9411 \
  jaegertracing/all-in-one:latest
```

Once up and running you can navigate to the default UI url http://localhost:16686/ to start viewing your logs. It order to get something showing up you will need to run the app and begin pushing some logs to Jaeger.

# Setting up New Relic

Everything is now setup from a code perspective, its time to create everything we need in New Relic. This step is quite quick and painless, a new key can be created from the API Keys section. Once there follow the **Create a Key** button. The form requires the following,

- Account - Set this to whatever you want
- Key Type - Set this to **Ingest - Licence**
- Name - Set this to whatever you want, however it does make sense to match this up with the service name set in the section "Configuring the Tracer"

![new-relic-adding-new-api-key](//images.ctfassets.net/wjg1udsw901v/3Eg3DL2escQx6jZdQsZYBQ/973a748bad74f91ecac6bd711f31f37a/new-relic-adding-new-api-key.png)

Once thats created you should get an option to copy the key which is passed into your `OTLPTraceExporter`. As for the URL you can find New Relics OTLP endpoints on this [link](https://docs.newrelic.com/docs/more-integrations/open-source-telemetry-integrations/opentelemetry/get-started/opentelemetry-set-up-your-app/). Now that we have the settings needed we can change the `OTLPTraceExporter` to include the New Relic settings.

```javascript 
const otlpExporter = new OTLPTraceExporter({
	url: 'https://otlp.nr-data.net:443',
	headers: {
	  "api-key": `API KEY GOES HERE`,
	},
});
```

Once setup you will need to send your first log before anything appears in New Relic. Once you have results will show up in APM & Services > Services - OpenTelemetry > Click on your app name > Distributed Tracing. You can drill down to view the span events and attributes.

New Relic also has a second method of viewing your logs through the use of NRQL queries. You can query your data by visiting **Query Your Data** and entering the following query,

```sql
SELECT *
FROM SpanEvent WHERE span.id IN ( 
	SELECT id FROM Span WHERE otel.library.name = 'YOUR APP NAME HERE'
)
```

And thats it, hope this was useful for someone! Before you get integrating note that there might be some logging providers that have a much easier integration with New Relic, [sentry.io](https://docs.sentry.io/platforms/javascript/guides/electron/) seems like one of them.13:T3098,As a engineer learning new languages, tools frameworks etc is just part and parcel of the job. Over time the spectrum of knowledge a full stack engineer has to learn has increased dramatically. Maintaining a cognitive memory across all the different CLI's, languages/frameworks etc in today's setting is no small feat.

Over time i've found that if you want something to stick you have to write it down, and if it doesn't stick (or un-sticks over time) you need some notes to look back at it. As my brain ðŸ§  ages these notes have become invaluable, and the need for quality notes over rushed pasted snippets has become more of a priority.

Due to this I decided to revisit and reassess how I learn, and in doing so I ended up discovering [Obsidian](https://obsidian.md/) which has been a game changer for me. This article is an engineers perspective of creating dev notes to aid in learning and recollection.

# My old setup

Prior to [Obsidian](https://obsidian.md/) my note taking activities all occurred in OneNote, I had both my personal notes as well as learning notes around different technologies all in one place split by sections.

![OneNote Structure](//images.ctfassets.net/wjg1udsw901v/o5aNPcsMQrTCGBp8ffbJY/454ecc55a38b36a53093897c1bc32181/OneNote_Structure.png)
_My programming category would contain a new section per technology_

The notes themself's contained a mixture of,

- Images (mostly diagrams)
- Paragraphs
- Code snippets (sure you can use gists or a repo, but i want context with my code snippets and i want it to be searchable)
- Links to articles, tools, documentation links ect.
- Youtube links usually tutorials

However I realized that there are some serious shortcomings with OneNote, and these problems are replicated across its competitors too, like Notion or Evernote. Heres my list,

- Code pasted in is never pure code, they always add markup on top to render which means copying and pasting out adds characters or unwanted proprietary formatting
- Diagrams are always disconnected, hard to edit. They are usually pasted in images from other tools.
- I don't have any version tracking, history or backup.
- Pasting and editing is distracting with as styling gets pasted in from the source.
- There are no connections in my notes, linking is not possible.
- I dont truly own any of the content, i will forever be tied to whatever I write in.

The list can go on, but you get the idea...

# How Obsidian makes it better

Obsidian is part of a next generation in IDE's designed specifically for research and note taking in their own words they describe it as,

> A second brain, for you, forever. Obsidian is a powerful knowledge base that works on top of a local folder of plain text Markdown files.

Its the VS code of markdown... no quite literally. At its base it provides a really smooth cross platform writing experience with some key features,

- Markdown editing, you get all the benefits of being able to easily style without leaving your keyboard, as well as encorperating code snippets easily.
- Tagging and creating links (backlinks) across your markdown files
- Graph view (more on that later)
- Powerful built in search

And just like VS code its got a rich plugin ecosystem that's powered by opensource github projects that greatly extend its usability and allows you to mold the app to your liking.

## You own what you write

Obsidian uses local markdown files that are stored on your own computers file system. There is no proprietary formatting, encoding etc, what you create is just standard markdown files (.md) and a bunch of folders.

![Obsidian File Structure](//images.ctfassets.net/wjg1udsw901v/7CdSonGKup7xhgpkBtucs7/5ff5b1baababda93c470bebee41bb464/Obsidian_File_Structure.png)

Why is this important you might ask?

- It means the notes you painstakingly curate are not locked into a single echo-system, you can shift between them as long as whatever your shifting to supports markdown.
- You also get greater control over how you backup and manage change tracking.
- You have greater control over the privacy of what you write, in a world full of breaches a shift away from centralized cloud storage is refreshing. _harshibar_ sums it up nicely with this video,

https://www.youtube.com/watch?v=HhWUjp5pD0g

## Works with git

Obsidian doesn't support git right out the box, it requires a community plugin called [Obsidian Git](https://github.com/denolehov/obsidian-git). However after having installed the plugin you end up with the greatest change tracking/archiving tool at your disposal.

![Obsidian Git](//images.ctfassets.net/wjg1udsw901v/gTn99qeodAoxHw1ybqvZg/a504f3b5e0547c47475d2071fb3ab076/Obsidian_Git.png)

Whilst the extension is fairly simple in that its mainly for pushing changes to a selected branch. It offers enough functionality to be out of your way. Dealing with more advanced scenarios like merge conflicts might need to resort to the CLI.

## Mermaid

As a visual thinker who thinks takes more information in from pictures than words the expression _a picture speaks a thousand words_ really hits home. In the past I would always use free tools like [draw.io](www/draw.io) to make diagrams then paste in the exported jpg's into my notes.

As a process I always found this really disconnected, and its even more jarring when you need to make edits and go through the whole export to jpeg flow all over again. I also find with these tools getting boxes to line up with connecting lines in a way that doesn't look a mess is incredibly fiddly and time consuming.

One later evolution of this was [Plant UML](https://plantuml.com/) which moves away from drawing diagrams to scripting, take this plantUML file as an example,

```
@startuml

left to right direction

actor Guest as g

package Professional {
  actor Chef as c
  actor "Food Critic" as fc
}

package Restaurant {
  usecase "Eat Food" as UC1
  usecase "Pay for Food" as UC2
  usecase "Drink" as UC3
  usecase "Review" as UC4
}

fc --> UC4
g --> UC1
g --> UC2
g --> UC3

@enduml
```

_Taken from the plantUML's [usecase examples](https://plantuml.com/use-case-diagram)_

The above script produces the following diagram, note that the formatting/spacing and styling is all governed by plantUML syntax... there is absolutely no drawing involved.

![PlantUML Example](//images.ctfassets.net/wjg1udsw901v/5jHzAIn4XYzmeeWbhn4yto/3154d97c8ed3f0c4456d3e7850ae6311/plantuml-example.png)

The benefit of this approach is you now have a change tracked diagram that can sit somewhere with your documentation (and as a bonus it formats everything for you). Aside from exporting the script as a .png making edits couldn't be easier.

Whilst PlantUML is great it still take you away from your markdown document as you need to separately create the diagram and later paste in a link to the exported image. A later evolution on this is [Mermaid](https://mermaid-js.github.io/mermaid/#/) which is similar to PlantUML but works within markdown as a code block. Having diagrams integrated means you no longer need to export diagrams to jpeg when adding to a markdown file... their just there. Here is an example,

```
stateDiagram-v2
[*] --> Still
Still --> [*]

Still --> Moving
Moving --> Still
Moving --> Crash
Crash --> [*]
```

_Taken from Mermaids [state diagram example](https://mermaid-js.github.io/mermaid/#/stateDiagram)_

The script produces the following diagram, note that this screenshot was taken directly from Obsidian as it supports Mermaid out right out the box.

![Mermaid Diagram Example](//images.ctfassets.net/wjg1udsw901v/3TdRha2qj11tFdfT1ULT7Y/725f09affa9ed12f05a62957ab88fc34/mermaid-diagram-example.png)

## Creating a open graph of knowledge!

Obsidian has the ability of collating all the connections through back-linking and tagging your document to produce a networked graph. Take a look at my graph so far,

![Obsidian Graph Zoomed Out](//images.ctfassets.net/wjg1udsw901v/pIYQ7RoXi4eYUW05zkrdn/d81b648ee4f721150b58c4c2fe46d643/obsidian-graph-zoomedout.png)

Here's another picture of it zoomed in, you can see all the connecting notes, as well all the connecting hash tags.

![Obsidian Graph Zoomed In](//images.ctfassets.net/wjg1udsw901v/3Ojk2IEqZ2d3qsT0sOliii/94ec4178cfb7d1f9c5e76a67e80b0d0e/obsidian-graph-zoomedin.png)

Admittedly whilst this looks really cool i haven't made much use out of this so far. It could be because I always target specific topics to learn and research into them rather than just free write so there's no real discovery of connections happening. In a team setting however the graph view could be really useful for content discovery especially when its cross cutting (like filtering on any article that's tagged with infrastructure).

# Plugins

Obsidian has a wide list of plugins, I thought it would be useful for someone new to Obsidian to get a sense of how its functionality can be extended. Below are some choice selections I use personally,

| Name                     | Description                                                                | Github Link                                                                          |
| ------------------------ | -------------------------------------------------------------------------- | ------------------------------------------------------------------------------------ |
| Advanced Tables          | Adds better navigation and formatting for markdown tables                  | [advanced-tables-obsidian](https://github.com/tgrosinger/advanced-tables-obsidian)   |
| Syntax Highlight         | Adds highlighting of code blocks within the editor view                    | [Syntax Highlight](https://github.com/deathau/cm-editor-syntax-highlight-obsidian)   |
| Markdown prettifier      | Fixes and formats your markdown file                                       | [Markdown prettifier](https://github.com/cristianvasquez/obsidian-prettify)          |
| Mind Map                 | Creates mind map view of your document                                     | [Mind Map](https://github.com/lynchjames/obsidian-mind-map)                          |
| Natural Language Dates   | Allows you to easily add dates into your markdown file                     | [Natural Language Dates](https://github.com/argenos/nldates-obsidian)                |
| Note Refactor            | Allows you to extract a piece of markdown into its own file                | [Note Refactor](https://github.com/lynchjames/note-refactor-obsidian)                |
| Obsidian Git             | Adds support for git, allows you to push changes to your repository        | [Obsidian Git](https://github.com/denolehov/obsidian-git)                            |
| Outliner                 | Adds better list manipulation, also allows you to fold lists               | [Outliner](https://github.com/vslinko/obsidian-outliner)                             |
| Paste URL into selection | Allows you to paste a link over text and auto format it                    | [Paste URL into selection](https://github.com/denolehov/obsidian-url-into-selection) |
| Reading Time             | More for the blog authors, gives you a reading time for your markdown file | [Readming Time](https://github.com/avr/obsidian-reading-time)                        |
| Recent Files             | Adds a panel showing recent opened files                                   | [Recent Files](https://github.com/tgrosinger/recent-files-obsidian)                  |
| Sliding Panes            | More of a UI change, allows you to slide between files                     | [Sliding Panes](https://github.com/deathau/sliding-panes-obsidian)                   |

# Themes

Obsidian also supports a bunch of themes and looks utterly glorious, I use a theme called _Atom_.

![Obsidian](//images.ctfassets.net/wjg1udsw901v/2Zj4rlGwr1ZaOxxVfb0eP7/968dcaae0e3ca7f49781ed334fe30424/Obsidian.png)

# Conclusion

Obsidian has really refreshed how I work to a modern standard. Hopefully this has given a good overview of the benefits from a engineers perspective.

The only caveat to Obsidian is the lack of mobile apps (its currently being worked on, check out there roadmap on [Trello](https://trello.com/b/Psqfqp7I/obsidian-roadmap)). But to be honest I haven't really felt the need for it as im always learning whilst on my computer.

Obsidian is free try use (and fully functional with its free tier), they also have some paid tiers and boltons, so do check it out!14:T9f3,I've been using C# for about a decade now, and every now and again I discover something that surprises me. This week it's the ability to deconstruct as we do in Javascript (and I'm not talking about using Tuples!).

Below is a simple example of deconstruction taking place to draw out the power, and defence property for our Trex object,

```
const trex = {
    statistics: {
        power: 10,
        defence: 2
    },
    name: "T-Rex", 
};

const { power, defence } = trex.statistics;

console.log(`Power ${power}, Defence ${defence}`);
//Power 10, Defence 2

//Better than doing:
//const power = trex.statistics.power;
//const defence = trex.statistics.defence;
```

As Mozilla's definition states,

> The destructuring assignment syntax is a JavaScript expression that makes it possible to unpack values from arrays, or properties from objects, into distinct variables - [Destructuring assignment](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Destructuring_assignment).

It's a powerful syntactical sugar, especially in the scenarios where you have a nested object with long names. As deconstruction can lead to cleaner, more readable code its uses are great on large object types. Now let's take a look at the same thing but in C#,

```
namespace deconstruction
{
    public record Statistics(int Power, int Defence);

    public class Trex
    {
        public Statistics Statistics;
        public string Name;

        public Trex()
        {
            Name = "T-Rex";
            Statistics = new Statistics(10, 5);
        }

        // Return the first and last name.
        public void Deconstruct(out int power, out int defence)
        {
            power = Statistics.Power;
            defence = Statistics.Defence;
        }
    }
}
```

Ok admittedly it's not as elegant as its Javascript counterpart as we need to define what we want to deconstruct upfront as well as have a function for each combination ðŸ˜¬! but it's still got its uses... check it out,

```
using System;

namespace deconstruction
{
    class Program
    {
        static void Main(string[] args)
        {
            var trex = new Trex();
            var (power, defence) = trex;

            Console.WriteLine($"Power: {power}, Defence: {defence}");
            //Power: 10, Defence: 5

            //Better than doing:
            //var power = trex.statistics.power;
            //var defence = trex.statistics.defence;
        }
    }
}
```

Who knows what other hidden gems ðŸ’Ž lie buried with the Microsoft docs!15:T4738,# Starting my journey with GraphQL

Up till now, I've always heavily relied on RESTfull services to power API's, this recently got widened with GRPC which you can read about in my article [.NET & GRPC What they forgot to tell you](https://www.faesel.com/blog/dotnet-grpc-forgot-to-tell-you). GraphQL was the third final frontier that needed exploring ðŸ¥¾...until now.

Having looked at it a year back the implementations for .NET were in their infancy, which meant that your server would only be as good as the framework you choose. Fast forward to 2021, [Chilli Creams Hotchocolate](https://github.com/ChilliCream/hotchocolate) has gained some serious ground and makes GraphQL an appealing proposition for developers.

In this article I hope to cover two main points,

- How REST is designed to break backend engineers
- How GraphQL saves the day

# Your typical REST scenario

Let's paint the scene, your a backend engineer who's creating an endpoint for showing a list of cats. With your battle-tested REST knowledge, you set out to create your first basic endpoint in the `CatsController` that returns all cats and the front end engineer is ready to integrate it into his UI.

```
// api/cats
[HttpGet]
public async Task<IActionResult> GetCats()
{
    using (var context = contextFactory.CreateDbContext())
    {
        var cats = await context.Cats.ToListAsync();

        if(cat != null)
            return Ok(cats);
    }

    return NoContent();
}
```

The app soon becomes a hit! your product manager decides to expand the functionality to filter by cat descriptions and to create a new cat information page. Getting to work you expand the endpoints for the front end engineers to use.

```
//api/cats/1
[HttpGet]
[Route("{id}")]
public async Task<IActionResult> GetCatsById([FromRoute] int id)
{
    using (var context = contextFactory.CreateDbContext())
    {
        var cats = await context.Cats.FirstOrDefaultAsync(x => x.Id == id);

        if(cats != null)
            return Ok(cats);
    }

    return NoContent();
}

// api/cats/description/brown
[HttpGet]
[Route("description/{description}")]
public async Task<IActionResult> GetCatsByDescription([FromRoute] string description)
{
    using (var context = contextFactory.CreateDbContext())
    {
        var cats = await context.Cats.Where(x => x.Description.Contains(description)).ToListAsync();

        if(cats != null)
            return Ok(payload);
    }

    return NoContent();
}
```

The cycle continues with product owners coming up with more feature requests and at the bottom of the pile, you got the backend engineer being reactive to all the changes. By the time you've wrapped up the project your left with a code smell of 10+ endpoints ðŸ’©.

The situation further degrades after a year when the UI gets redesigned and features are culled based on user usage. You end up with random floating endpoints because quite frankly no one audits their endpoints for dead code.

This is where GraphQL steps in, it switches the responsibility of an engineer from anticipating and creating endpoints to simply upfront displaying everything that's available with declarative meaning.

# Hot Chocolate (.NET GraphQL server framework)

Hot chocolate is one of the leading implementations of a GraphQL server, one important thing to note when choosing a framework is that your implementation will only be as good as the framework you choose. As the [GraphQL specification](http://spec.graphql.org/) progresses you want a framework that keeps up to date with the changes... Hot Chocolate does that.

To understand the basics of Hot Chocolate I recommend [**Les Jackson's**](https://www.youtube.com/user/binarythistle) free course on youtube. It is a bit lengthy at 3 Hours and 45 Minutes but it allows you to create an ASP.NET implementation from scratch and understand basic concepts like Querys, Mutations and Subscriptions. By the end of the course, you have a GraphQL service that can do CRUD actions (do ðŸ‘ his video it's great!).

https://www.youtube.com/watch?v=HuN94qNwQmM

The source code he produces can also be found on his [github repo](https://github.com/binarythistle/S04E01---.NET-5-GraphQL-API). The source code is a great starting point as it creates a docker image containing an MSSQL database. The solution itself already has Entity Framework and Hot Chocolate bootstrapped, with two entities to test with.

On top of this [Banana Cake Pop ðŸŒ](https://github.com/ChilliCream/hotchocolate/blob/main/src/BananaCakePop) is also integrated which allows you to query your server through a browser (similar to swagger).

As well as [GraphQL Voyager ðŸš€](https://github.com/APIs-guru/graphql-voyager) (do checkout the [live demo](https://apis.guru/graphql-voyager/)).

To understand the remainder of the article it's important to have some basic knowledge of Hot Chocolate.

# GraphQl Voyager

Whilst this is an addition to what's being discussed, it's worth briefly mentioning. Voyager helps facilitate the move of a backend engineer from creating and documenting prescriptive REST endpoints to simply becoming a harbour of documentation and entities.

>The marker of a quality API has shifted from creating a subjectively RESTfull API and how well it's documented to ... just how well it's documented ðŸ“.

Here's a taste of what is looks like for our API,

![Graphql Voyager](//images.ctfassets.net/wjg1udsw901v/8f7Kb65mDXS5ZBSA4At50/08bcdb6bfe661bdb50965e7b9e8d5a81/graphql-voyager.png)

# What they forgot to mention

Up till now what we have discussed fits the 80% CRUD usecase, however as we know API's that are in the wild also deal with a range of other responsibilities. The remainder of this article is to shed some light on how this is done.

## How to version your API

The typical versioning strategy for REST is to version using URLs `https://api.cats.com/v1` (when developers can be bothered). However with GraphQL as your only ever posting to a single endpoint that strategy is no longer a prefered solution.

>While there's nothing that prevents a GraphQL service from being versioned just like any other REST API, GraphQL takes a strong opinion on avoiding versioning by providing the tools for the continuous evolution of a GraphQL schema.
*[Taken from GraphQL Best Practices](https://graphql.org/learn/best-practices/)*

Before we begin on how to version, there are some distinct points to note.

Non-breaking changes can continue as they would with REST, adding properties to entities (as you would with response models in REST), continues to be a way to evolve your API. Similarly adding new query types to your GraphQL server is also deemed as a non-breaking change, and is equivalent to adding new endpoints in REST.

GraphQL aids in breaking changes caused due to nullability as **everything** unless specified is treated as nullable. This leads to upfront resilience on the front end to missing data, the `id: Int!` in the example below cannot be null.

```
type Cats {
  id: Int!
  name: String
}
```

Eventually, we do still hit circumstances where a breaking change is needed. In these situations we have two strategies. The first as [Chilli Cream Docs](https://chillicream.com/docs/hotchocolate/defining-a-schema/versioning/) specify is to add deprecated flags to old properties and begin to shift usage to new versions.

```

public class CatsType : ObjectType<Cats>
{
    protected override void Configure(IObjectTypeDescriptor<Cats> descriptor)
    {
        descriptor.Description("Represents commands available on a platform");

        descriptor.Field(x => x.Name).Deprecated("This is no longer used, use FirstName and LastName");
        descriptor.Field(x => x.FirstName);
        descriptor.Field(x => x.LastName);
    }
}
```

For client developers, this then creates warnings when using a deprecated property.

![Depricated GraphQL property](//images.ctfassets.net/wjg1udsw901v/2BkWWBcz1UAAU9fFVavBGQ/c812ee032ddcd8eb600b165616f68e5b/Screenshot_2021-03-26_153337.png)

Whilst this approach works over time it could create a lot of noise if you have many deprecated properties, an alternative approach is to split the entity entirely, use different classes between the two versions. Here is an example, we start by creating two query types,

```
public record CatResponse1(int Id, string Name);
public record CatResponse2(int Id, string FirstName, string LastName);
```

Both of these would contain their own Code First type files

```
public class CatType1 : ObjectType<CatResponse1>
{
    protected override void Configure(IObjectTypeDescriptor<CatResponse1> descriptor)
    {
        descriptor.Description("Represents cats!");

        descriptor.Field(x => x.Name)
            .Description("Represents the name of the cat")
            .Deprecated("This is no longer used, use FirstName and LastName from Cat2");
    }
}

public class CatType2 : ObjectType<CatResponse2>
{
    protected override void Configure(IObjectTypeDescriptor<CatResponse2> descriptor)
    {
        descriptor.Description("Represents cats!");

        descriptor.Field(x => x.FirstName)
            .Description("Represents the name firstname of the cat");
        descriptor.Field(x => x.LastName)
            .Description("Represents the name lastname of the cat");
    }
}
```

The final piece of code is to use the intermediary response models. Under the hood we are still using the same EF entity.

```
public class Query
{
    [UseDbContext(typeof(AppDbContext))]
    [UseFiltering]
    [UseSorting]
    public IQueryable<CatResponse1> GetCat1([ScopedService] AppDbContext context)
    {
        var cats = context.Cats;

        return cats.Select(x => new CatResponse1(x.Id, x.Name));
    }

    [UseDbContext(typeof(AppDbContext))]
    [UseFiltering]
    [UseSorting]
    public IQueryable<CatResponse2> GetCat2([ScopedService] AppDbContext context)
    {
        var cats = context.Cats;

        return cats.Select(x => new CatResponse2(x.Id, x.FirstName, x.LastName));
    }
}

```

These changes now allow us to split our models, the two can be queried independently.

![Versioned types](//images.ctfassets.net/wjg1udsw901v/4RSPAHISSIpKS3qFfZbRq1/b53386073d0e86bc0e14a95b30ff857a/versioned-entities.png)

## How to do Authentication

Since Hot Chocolate works on top of ASP.NET we can leverage on all the traditional Authentication pipelines we use for REST, nothing changes! To demonstrate this I'm going to extend the base implementation with a basic authentication mechanism using a header value `x-api-key` and a key defined in the `appsettings.json`.

### Adding key-based authentication

To begin let's first add basic app settings to hold our authentication key (this represents the key the client will pass to the server to authentication their request), and create a class to deserialise into using `IOptions` interface.

```
//Code goes into appsettings.json
"AuthenticationSettings": {
    "AuthenticationToken": "secret123"
}

//New class to serialize into
public class AuthenticationSettings
{
    public string AuthenticationToken { get; set; }
}

//Register the configuration in Startup.cs > ConfigureServices function
services.Configure<AuthenticationSettings>(Configuration.GetSection(nameof(AuthenticationSettings)));
```

Next we will create authentication scheme options as follows,

```
public class ApiKeyAuthenticationOptions : AuthenticationSchemeOptions
{
    public const string DefaultScheme = "KeyBasedScheme";
    public string Scheme => DefaultScheme;
    public string AuthenticationType = DefaultScheme;
}
```

The next part is where the crux of the code is, the `AuthenticationHandler` is what determines whether the request was correctly authenticated. On a successful attempt, it populates the ClaimsPrinciple.

```
public class ApiKeyAuthenticationHandler : AuthenticationHandler<ApiKeyAuthenticationOptions>
{
    private const string ProblemDetailsContentType = "application/problem+json";
    private const string AuthenticationHeaderName = "x-api-key";
    private readonly AuthenticationSettings AuthenticationSettings;

    public ApiKeyAuthenticationHandler(
        IOptionsMonitor<ApiKeyAuthenticationOptions> options,
        ILoggerFactory logger,
        UrlEncoder encoder,
        ISystemClock clock,
        IOptions<AuthenticationSettings> authenticationSettings) : base(options, logger, encoder, clock)
    {
        AuthenticationSettings = authenticationSettings.Value;
    }

    protected override Task<AuthenticateResult> HandleAuthenticateAsync()
    {
        if (!Request.Headers.TryGetValue(AuthenticationHeaderName, out var apiKeyHeaderValues))
        {
            return Task.FromResult(AuthenticateResult.NoResult());
        }

        var providedApiKey = apiKeyHeaderValues.FirstOrDefault();

        if (apiKeyHeaderValues.Count == 0 || string.IsNullOrWhiteSpace(providedApiKey))
        {
            return Task.FromResult(AuthenticateResult.NoResult());
        }

        var isMatchingKey = providedApiKey.Equals(AuthenticationSettings.AuthenticationToken);

        if (isMatchingKey)
        {
            var claims = new List<Claim> {
                //Add your claims here
            };
            var identity = new ClaimsIdentity(claims, Options.AuthenticationType);
            var identities = new List<ClaimsIdentity> { identity };
            var principal = new ClaimsPrincipal(identities);
            var ticket = new AuthenticationTicket(principal, Options.Scheme);

            return Task.FromResult(AuthenticateResult.Success(ticket));
        }

        return Task.FromResult(AuthenticateResult.Fail("Invalid API Key provided."));
    }

    protected override async Task HandleChallengeAsync(AuthenticationProperties properties)
    {
        Response.StatusCode = (int)HttpStatusCode.Unauthorized;
        Response.ContentType = ProblemDetailsContentType;
        var problemDetails = new { Information = "Unauthorized" };

        await Response.WriteAsync(JsonSerializer.Serialize(problemDetails));
    }

    protected override async Task HandleForbiddenAsync(AuthenticationProperties properties)
    {
        Response.StatusCode = (int)HttpStatusCode.Forbidden;
        Response.ContentType = ProblemDetailsContentType;
        var problemDetails = new { Information = "Forbidden" };

        await Response.WriteAsync(JsonSerializer.Serialize(problemDetails));
    }
}
```

The final part need is to register this in our startup class, below are the two bits of code needed. Once we have this in place the `[Authorize]` tag will work for regular REST requests, any request sent without an `x-api-key` value of 'secret123' will be rejected. The next step is to see how we replicate this in GraphQL.

```
public void ConfigureServices(IServiceCollection services)
{
    ...
    services.AddAuthentication(options =>
    {
        options.DefaultAuthenticateScheme = ApiKeyAuthenticationOptions.DefaultScheme;
        options.DefaultChallengeScheme = ApiKeyAuthenticationOptions.DefaultScheme;
    }).AddScheme<ApiKeyAuthenticationOptions, ApiKeyAuthenticationHandler>(
        ApiKeyAuthenticationOptions.DefaultScheme,
        null
    );
    services.AddAuthorization();
}

public void Configure(IApplicationBuilder app, IWebHostEnvironment env)
{
    ...
    app.UseAuthentication();
    app.UseAuthorization();
}
```

### Authenticating a GraphQl Entity

Authentication in GraphQL works by authorizing individual models, to begin we first need to add the HotChocolate Authorization package `HotChocolate.AspNetCore.Authorization` and enable it in the `Startup.cs` class, its a one-liner,

```
services.AddAuthorizeDirectiveType()
```

Now similar to the `[Authorize]` tag we use for REST we can enable Authorization in for our individual ObjectTypes by adding a simple `descriptor.Authorize()` call.

```
public class CatType1 : ObjectType<CatResponse1>
{
    protected override void Configure(IObjectTypeDescriptor<CatResponse1> descriptor)
    {
        descriptor.Authorize();
        ...
    }
}
```

Once this has been added making calls without the header will return an unauthenticated result that looks like this,

```
{
  "errors": [
    {
      "message": "The current user is not authorized to access this resource.",
      "locations": [
        {
          "line": 3,
          "column": 5
        }
      ],
      "path": [
        "cat1",
        1,
        "id"
      ],
      "extensions": {
        "code": "AUTH_NOT_AUTHENTICATED"
      }
    },
    ...
```

## How to do Authorisation

Extending the code to work with Authorization is also a quick change, in this example we will authorize based on the user's role. To begin we will extend our `ApiKeyAuthenticationHandler` to populate a claim when the authentication key has matched,

```
var claims = new List<Claim> {
    new Claim("http://schemas.microsoft.com/ws/2008/06/identity/claims/role", "Admin")
};
```

Next we can pass a list of accepted roles into the ObjectType `CatType1`, in this example I have intentionally added a role that doesn't exist.

```
var roles = new string[] { "NotAdmin" };
descriptor.Authorize(roles);
```

Making a request now will spark an unauthorized error,

```
{
  "errors": [
    {
      "message": "The current user is not authorized to access this resource.",
      "locations": [
        {
          "line": 3,
          "column": 5
        }
      ],
      "path": [
        "cat1",
        1,
        "id"
      ],
      "extensions": {
        "code": "AUTH_NOT_AUTHORIZED"
      }
    },
    ...
```

## How does logging work

Regarding logging Chilli Cream has created a guide to adding an `AddDiagnosticEventListener` that's able to trace incoming requests, check out the article [Log Your Queries While Building a GraphQL Server](https://chillicream.com/blog/2021/01/10/hot-chocolate-logging). It would be interesting to create an example that's `OpenTelemetry` compliant... perhaps that's one for another day (this articles getting a bit long ðŸ˜©).

# Conclusion

That's it folks! We've seen that the Hot Chocolate implementation nicely fulfils not just the 80% crud use case but can also deal with the other responsibilities we typically see with our REST services in the wild.16:T3753,Recently I started working on a project that was created from the **ASP.NET SPA template for react**. It's one of the templates you get by default with dotnet and can be created by running `dotnet new react`.

The template creates a dotnet webapp which is designed to be an API backend and links it with a react project to power the UI. When running the project from dotnet, static files are built from the react project and served up.

In terms of running the application with different environments, the dotnet perspective is fairly straight forward as we can simply use the environment variable `ASPNETCORE_ENVIRONMENT`. But the question is how do we pass this variable to the SPA so that we can shift between different environments?

Having trawled the internet I didn't see any examples, so I decided to create my own!

# Understanding the ASP.NET spa template ðŸ”
Let's begin by creating a boilerplate solution with `dotnet new react`. Once the solution is created we end up with a backend API with a `WeatherForecastController`, and a front end app located in the `ClientApp` folder.

![Dotnet SPA Folder Structure](//images.ctfassets.net/wjg1udsw901v/65iH6wUZxUc6JSNON5gWTK/a704de8ffa2dd0d4a33eef19ebf92390/folder-structure.png)

Since this is an integrated spa, from the root of the project we are able to `dotnet run` and spin up not only the dotnet project but also the react spa.

## Client App

The client app itself is in a completely segregated app, there's nothing special added here to make it all connect up. All your standard commands to `npm install/build` are all available to you. In fact, the template has been build based on the implementation of `create-react-app`.

You can also start up the project from here with `npm run start` command which will spin up a development server **independent of your backend code**. The execution and configuration is handled for us using `react-scripts` which was designed to help set up react projects without stress, featuring things like hot module reloading, deployment builds etc ... all standard-issue so far. So you get these npm scripts setup for you,

```
  "scripts": {
    "start": "rimraf ./build && react-scripts start",
    "build": "react-scripts build",
    "test": "cross-env CI=true react-scripts test --env=jsdom",
    "eject": "react-scripts eject",
    "lint": "eslint ./src/"
  },
```

## Startup Class

The glue that connects the backend to the frontend can be found in the `Startup.cs` class. Working from top down the first code block of interest is within the *ConfigureServices* function,

```
services.AddSpaStaticFiles(configuration =>
{
  configuration.RootPath = "ClientApp/build";
});
```

This block essentially tells your dotnet app where to find the static resources (production builds) of your spa within its bin folder. So running the command `dotnet publish --configuration Release` creates a **ClientApp/Build** folder with a production optimised (ie npm run build) version of our SPA, the root path simply points to this.

![clientapp-build-folder](//images.ctfassets.net/wjg1udsw901v/4tIwYVA6pYWrGctHAzMXHT/d0836b601480a1962a7693bf4cd8b653/spa-build-folder.png)

The next block to notice is in the *Configure* function,

```
app.UseEndpoints(endpoints =>
{
    endpoints.MapControllerRoute(
        name: "default",
        pattern: "{controller}/{action=Index}/{id?}");
});

app.UseSpa(spa =>
{
    spa.Options.SourcePath = "ClientApp";

    if (env.IsDevelopment())
    {
        spa.UseReactDevelopmentServer(npmScript: "start");
    }
});
```

There are two things that are happening here, the first is that we have dotnet server side routing connected up (with app.UseEndpoints() middleware), this means that upon receiving a HTTP request server-side routing will always take priority over client-side routing. If server-side routes fall through without matching an endpoint, we use the app.UseSpa() middleware to redirect all requests to the default page (which is your index.html file triggering the spa to load).

The next point is that from here we can also configure the location of our client-side source code, and the command we need to use to run our react spa as a development server when debugging.

## MSBuild & Running NPM Commands

The remaining magic is all located in the .csproj file we got 2 core components here the first is the Debug target,

```
<Target Name="DebugEnsureNodeEnv" BeforeTargets="Build" Condition=" '$(Configuration)' == 'Debug' And !Exists('$(SpaRoot)node_modules') ">
  <!-- Ensure Node.js is installed -->
  <Exec Command="node --version" ContinueOnError="true">
    <Output TaskParameter="ExitCode" PropertyName="ErrorCode" />
  </Exec>
  <Error Condition="'$(ErrorCode)' != '0'" Text="Node.js is required to build and run this project. To continue, please install Node.js from https://nodejs.org/, and then restart your command prompt or IDE." />
  <Message Importance="high" Text="Restoring dependencies using 'npm'. This may take several minutes..." />
  <Exec WorkingDirectory="$(SpaRoot)" Command="npm install" />
</Target>
```

This chunky block of code runs an `npm install` command before building your dotnet application. It also features a nice check to ensure you got Node.js installed (I guess for the backend people ðŸ˜). It does this with the `<Exec WorkingDirectory="$(SpaRoot)" Command="npm install" />` command runner (note SpaRoot is defined a the top as a static property pointing to **ClientApp\**).

The second part is the publish target,

```
<Target Name="PublishRunWebpack" AfterTargets="ComputeFilesToPublish">
  <!-- As part of publishing, ensure the JS resources are freshly built in production mode -->
  <Exec WorkingDirectory="$(SpaRoot)" Command="npm install" />
  <Exec WorkingDirectory="$(SpaRoot)" Command="npm run build" />

  <!-- Include the newly-built files in the publish output -->
  <ItemGroup>
    <DistFiles Include="$(SpaRoot)build\**" />
    <ResolvedFileToPublish Include="@(DistFiles->'%(FullPath)')" Exclude="@(ResolvedFileToPublish)">
      <RelativePath>%(DistFiles.Identity)</RelativePath>
      <CopyToPublishDirectory>PreserveNewest</CopyToPublishDirectory>
      <ExcludeFromSingleFile>true</ExcludeFromSingleFile>
    </ResolvedFileToPublish>
  </ItemGroup>
</Target>
```

Again fairly similar concept running a publish first installs dependencies then builds the project. The build artefacts get created in the **ClientApp\build** folder. The item group block then ensures the build assets are included in your **bin\ClientApp\build** folder.

## Summary

So to summarise, when running in debug mode

1. We npm install dependencies
2. Build and run the dotnet app
3. Run an npm development server
4. Begin routing all calls to the backend, and where it fails to the default client page.

In the case of a published application

1. We create a published version of the dotnet application
2. We npm install dependencies
3. We create a production build of the spa in the folder **ClientApp\build**
4. Static files from the spa are included in the output
5. Running the application now has a packaged version of the spa its static files are served up upon running the backend. 

This is all well and great so far... but the react app gets pre built.. what if we need to run it as part of a different environment? Currently, it's all running off a single `.env` file!

# Adding environments ðŸ†•

Below is my solution for getting environments running across the stack, it also conforms to the dev-ops ethos of,

> Build once and deploy many times.

## Install the dependencies

To being adding environments we first need to ensure our npm app can support it. For this, we will use the well know [env-cmd](https://www.npmjs.com/package/env-cmd). 

We will also be needing something to manipulate the build folders generated by react-scripts. Since all operating systems are equipped with CLI commands to rename/remove files we don't need anything special to do this. However, because these commands differ from one operating system to the next, it's always a good practice to use something like [shx](https://www.npmjs.com/package/shx) to ensure it works cross-platform.

So let's start with running the install command in the ClientApp folder,

`npm install env-cmd shx --save-dev`

## Add your environment files

Next let's start creating some environment files, the file structure should look something like this, with the .env file containing settings shared across all the environments:

- .env
- .env.staging
- .env.production

Any key on these files need to be prefixed with `REACT_APP_` this is a safety feature build in,

> You must create custom environment variables beginning with REACT_APP_. Any other variables except NODE_ENV will be ignored to avoid accidentally exposing a private key on the machine that could have the same name.

For now, let's add just add an environment variable that tells us which environment we are in. Do this for both production and staging .env files.

```
REACT_APP_ENV='production'
```

To show our environment on the page lets also create a *config.js* file, that accesses the environment variable.

```
export const config = {
    ENVIRONMENT: process.env.REACT_APP_ENV 
};
```

And finally output it to the page,

```
import React, { Component } from 'react';
import { Container } from 'reactstrap';
import { NavMenu } from './NavMenu';
import { config } from '../config';

export class Layout extends Component {
  static displayName = Layout.name;

  render () {
    return (
      <div>
        {config.ENVIRONMENT}
        <NavMenu />
        <Container>
          {this.props.children}
        </Container>
      </div>
    );
  }
}

```

## Add your build scripts

Build scripts are now needed to trigger the environments, we need to make the following amends to the npm scripts section,

```
  "scripts": {
    "build:staging": "env-cmd -f .env.staging react-scripts build && shx rm -rf staging && shx cp -r build staging",
    "build:production": "env-cmd -f .env.production react-scripts build && shx rm -rf production && shx cp -r build production",
    "start:staging": "rimraf ./build && env-cmd -f .env.staging react-scripts start",
    "start:production": "rimraf ./build && env-cmd -f .env.production react-scripts start",
  }
```

The scripts prefixed with *build* are using env-cmd with its respective environment file to create a production build of the app. The shx part is then firstly removing the folder staging/production then copying the *build* files react-script creates into an environment specific folder.

Similarly the scripts prefixed with *start* run the app using a certain environment. Note if your trying to run this from the dotnet app, you will need to change the Startup.cs > UseReactDevelopmentServer function to,

```
spa.UseReactDevelopmentServer(npmScript: "start:production");
```

Now that this is set up, running the app should show the environment variables.

## Modifying your .csproj

The next step is to get this working with `dotnet publish`! To do this we need to modify the **PublishRunWebpack** target in the .csproj file to,

```
<Target Name="PublishRunWebpack" AfterTargets="ComputeFilesToPublish">
  <Exec WorkingDirectory="$(SpaRoot)" Command="npm install"/>

  <Message Importance="high" Text="Started building staging version of the spa ..." Condition=" '$(Configuration)' == 'Release' "/>
  <Exec WorkingDirectory="$(SpaRoot)" Command="npm run build:staging" Condition=" '$(Configuration)' == 'Release' "/>
  <Message Importance="high" Text="Started building production version of the spa ..." Condition=" '$(Configuration)' == 'Release' "/>
  <Exec WorkingDirectory="$(SpaRoot)" Command="npm run build:production" Condition=" '$(Configuration)' == 'Release' "/>

  <Exec WorkingDirectory="$(SpaRoot)" Command="npm run build" Condition=" '$(Configuration)' == 'Debug' " />
  <ItemGroup>
    <DistFiles Include="$(SpaRoot)build\**" Condition=" '$(Configuration)' == 'Debug' " />
    <DistFiles Include="$(SpaRoot)staging\**" Condition=" '$(Configuration)' == 'Release' " />
    <DistFiles Include="$(SpaRoot)production\**" Condition=" '$(Configuration)' == 'Release' " />
    <ResolvedFileToPublish Include="@(DistFiles->'%(FullPath)')" Exclude="@(ResolvedFileToPublish)">
      <RelativePath>%(DistFiles.Identity)</RelativePath>
      <CopyToPublishDirectory>PreserveNewest</CopyToPublishDirectory>
      <ExcludeFromSingleFile>true</ExcludeFromSingleFile>
    </ResolvedFileToPublish>
  </ItemGroup>
</Target>
```

To summarise what's happening here, when building in debug mode we are continuing to use the `npm run build` command to create a production build and the spa files get stored in the build folder `$(SpaRoot)build\**`. The output looks like this:

- bin
  - Release
    - publish
      - ClientApp
        - build
          -spa files go here!   

However in release mode we now create two versions of the spa (one for each environment) using out new npm environment builds, `npm run build:staging` and  `npm run build:production`. The builds also get moved to their corresponding folders.

- bin
  - Release
    - publish
      - ClientApp
        - staging
          - staging spa files go here!
        - production
          - production spa files go here!

Once thats setup you can test it out with `dotnet publish --configuration Release`, the build output should look something like this,

![build-output-spa](//images.ctfassets.net/wjg1udsw901v/6gBCaZH5bJ6vaNW2wf8QPD/7b1fcf59b5a716760ebf49c95aaaab85/BuildExample.png)

## Modifying your startup.cs

The final step is to modify your Startup.cs file to switch out which spa to use based on the environment variable,

```
services.AddSpaStaticFiles(configuration => configuration.RootPath = WebHostEnvironment.IsDevelopment()
                ? "ClientApp/build"
                : $"ClientApp/{WebHostEnvironment.EnvironmentName}");
```

# How to deploy ðŸš€

Deployment is now a simple case of running `dotnet publish --configuration Release`, once the published artefacts are deployed the app can now take its environment and run the appropriate spa. Build once and deploy anywhere!
17:T3635,# Why I started building a CLI

As a .NET engineer, I work with Azure storage a lot, its versatility, ease of use, as well as cost makes it a common staple amongst developers. Its application is also widespread from leveraging queues on a basic console app to storing uploaded images from a web application.

Typically as an engineer, I have always interfaced with azure storage using [Azure Storage Manager](https://azure.microsoft.com/en-gb/features/storage-explorer/#features), but as a UI tool, its always been two clicks away from the information I need or was just slow to navigate. 

So I ended up taking my destiny into my own hands and built a CLI tool called [Az-Lazy](https://github.com/faesel/az-lazy).

# Packages used when I started

So as with all projects, I started with a shopping list of packages I wanted to use/needed to make a great CLI experience. 

### 1. [CommandLineParser](https://github.com/commandlineparser/commandline)

Whilst there are many command parsers on the market, I found this implementation particular nice to use. The end result allows you to to get a command pattern similar to most of the Microsoft dotnet tools,  `azlazy connection --list`.

Defining available commands is as easy as decorating a class with attributes, and the help options for each command is automatically generated for you (azlazy addcontainer --help).

```
[Verb("addcontainer", HelpText = "Creates a new storage container")]
    public class AddContainerOptions : ICommandOptions
    {
        [Option('n', "name", Required = true, HelpText = "Name of the container to create")]
        public string Name { get; set; }

        [Option('p', "publicAccess", Required = false, HelpText = "Options are None, Blob, BlobContainer")]
        public string PublicAccess { get; set; }
    }
```

### 2. [Pastel](https://github.com/silkfire/Pastel)

Splash of colour is always a sign of a great CLI experience and seeing red or green are key indicators of successful execution of a command. *Pastel* lets you do exactly that! its got a range of preset vibrant colours to choose from and allows you to easily RGB your console output. 

The extension method style syntax is also great and doesn't distract you away from the code.

```
"You successfully deleted all your data".Pastel(Color.LightGreen);
```

### 3. [ConsoleTables](https://github.com/khalidabuhakmeh/ConsoleTables)

As Az-Lazy deals with table storeage, it was only a matter of time till i needed to output a table to the console. After a quick search on Nuget Gallery *ConsoleTables* showed up. With a simple minimilist table implementation it was quick to get setup,

```
var table = new ConsoleTable("Id", "Blob", "Size");
table.AddRow(1, "dinopic1.jpg", 300)
     .AddRow(2, "dinopic2.jpb", 450);

table.Write();
```

However, I later had to drop this package for something slightly more advanced (Alba.CsConsoleFormat) as I needed to word wrap large cells (particularly when displaying JSON snippets). 

### 4. [Alba.CsConsoleFormat](https://github.com/Athari/CsConsoleFormat)

As mentioned above *Alba.CsConsoleFormat* was a replacement package to render tables. With great options to not only style your table with colours but also specify word wrap options, it was a good choice for rendering large volumes of data.

I did find its syntax a little verbose to work with,

```
var headerThickness = new LineThickness(LineWidth.Double, LineWidth.Single);

var doc = new Document(
    new Span("Dinosaurs #") { Color = Yellow }, Order.Id, "\n",
    new Span("Type: ") { Color = Yellow }, Order.Customer.Name,
    new Grid {
        Color = Gray,
        Columns = { GridLength.Auto, GridLength.Star(1), GridLength.Auto },
        Children = {
            new Cell("Id") { Stroke = headerThickness },
            new Cell("Name") { Stroke = headerThickness },
            new Cell("Count") { Stroke = headerThickness },
            Order.OrderItems.Select(item => new[] {
                new Cell(item.Id),
                new Cell(item.Name),
                new Cell(item.Count) { Align = Align.Right },
            })
        }
    }
);

ConsoleRenderer.RenderDocument(doc);
```

Buy my oh my does it produce a great console table ðŸ‘¨â€ðŸŽ¨

![CsConsoleFormat table output](//images.ctfassets.net/wjg1udsw901v/tfEbNlJ8U1oqZCdmXGfq0/7d9eae3125e0c3053c4dfe91335a104e/peekmessages.png)

### 5. [LiteDb](https://github.com/mbdavid/LiteDB)

One of the requirements of Az-Lazy was to store a list of connections to be reused at any time. Since CLI's don't have any state, I needed a lightweight storage mechanism that can easily ship with the tool.

In comes *LiteDb*! With its entity framework style CRUD syntax, I really felt at home with this framework,

```
// Create your POCO class
public class Dinosaur
{
    public int Id { get; set; }
    public string Name { get; set; }
    public int Age { get; set; }
}

using(var db = new LiteDatabase(@"DinoDb.db"))
{
    var collection = db.GetCollection<Dinosaur>("dinosaurs");

    var dinosaur = new Dinosaur
    { 
        Id = 1
        Name = "T-Rex", 
        Age = 39
    };

    collection.Insert(dinosaur);

    var oldDinosaurs = col.Find(x => x.Age > 50);
}
```

Since *LiteDb* stores all its data into one file, from a tool perspective it ensures your not littering your client's computer with files.

# Packages used now

### 1. [CommandLineParser](https://github.com/commandlineparser/commandline)

I've still continued to use *CommandLineParser* however it's starting to hit some limitations, namely nesting of commands. So something like `azlazy connection add --name "test" --connection "connectionString"` is not allowed.

Whilst this package has been great to get me started, I might start looking at other options as Az-Lazy starts supporting more complex commands.

### 2. [LiteDb](https://github.com/mbdavid/LiteDB)

It just works, still happy with this package. Really recommend it for CLI tools!

### 2. [Spectre.Console](https://github.com/spectresystems/spectre.console)

*Spectre.Console* was a big find for me, from the writer of [Cake](https://github.com/cake-build/cake) ([Patrik Svensson](https://github.com/patriksvensson)) it's a one-stop-shop for all your CLI needs. Just to name a few of the features,

- Console Colours
- Progress bars
- Tables
- Prompts
- Spinners

After discovering this package I ended up doing a NuGet cull ðŸª“ which is why this list now stops at 3 (That culls still in progress as I'm removing Pascal for Specters implementation).

I won't go into all the code samples for this, but here are a few, beginning with progress indicators (also note the syntax to colour the output).

```
await AnsiConsole.Progress()
    .StartAsync(async ctx =>
    {
        // Define tasks
        var dinoTask = ctx.AddTask("[green]Uploading dinosaurs[/]");

        while (!ctx.IsFinished)
        {
            // Simulate some work
            await Task.Delay(250);
            dinoTask.Increment(1.5);
        }
    });
```

Check out how it looks in action, it's great for long-running tasks.

![Upload progress bar](//images.ctfassets.net/wjg1udsw901v/7I5IMPuMYHvUIUyan3iNLY/d7b5e472fed4daba62218d4d2d3165ee/uploaddirectory.png)

Table's are also strightforward to create,

```
// Create a table
var table = new Table();

// Add some columns
table.AddColumn("Foo");
table.AddColumn(new TableColumn("Bar").Centered());

// Add some rows
table.AddRow("Baz", "[green]Qux[/]");
table.AddRow(new Markup("[blue]Corgi[/]"), new Panel("Waldo"));

// Render the table to the console
AnsiConsole.Render(table);
```

# Other great code snippets

The only thing lacking for me in *Specter.Console* was display a tree structure which is useful for folder hierarchies, (there is an open issue here if you want to [show your interest](https://github.com/spectresystems/spectre.console/issues/144)) ðŸ™.

To fulfil this requirement, I found a great article by Andrew Lock which renders this structure, [Creating an ASCII-art tree in C#](https://andrewlock.net/creating-an-ascii-art-tree-in-csharp/). Here's what it looks like for me,

![Printing Tree structure](//images.ctfassets.net/wjg1udsw901v/6DeIXSOgHE0DO7STrDJPrw/281dd0130f40d8764dc9cdb6e3933509/containertree.png)

# Why I chose .NET

So initially I was torn between Node JS and C#, there was an especially compelling article by [Twilio](https://www.twilio.com/blog/how-to-build-a-cli-with-node-js) outlining how you can create a great CLI experience that was winning me over. They also went so far as suggesting some interesting packages to make use of, here's a snippet of whats comparable with what I used for az-lazy.

- [inquirer](http://npm.im/inquirer), [enquirer](http://npm.im/enquirer) or [prompts](https://npm.im/prompts) for complex input prompts
- [chalk](http://npm.im/chalk) or [kleur](https://npm.im/kleur) for colored output
- [ora](http://npm.im/ora) for beautiful spinners
- [boxen](http://npm.im/boxen) for drawing boxes around your output
- [stmux](http://npm.im/stmux) for a tmux like UI
- [listr](http://npm.im/listr) for progress lists
- [meow](http://npm.im/meow) or [arg](http://npm.im/arg) for basic argument parsing
- [commander](http://npm.im/commander) and [yargs](https://www.npmjs.com/package/yargs) for complex argument parsing and subcommand support
- [oclif](https://oclif.io/) a framework for building extensible CLIs by Heroku ([gluegun](https://infinitered.github.io/gluegun/#/) as an alternative)

Whilst the NuGet ecosystem is not as diverse as npm, after discovering *Spectre.Console*, I found that its a one-stop-shop for most of those npm packages mentioned above.

The deciding factor for me however was down to the userbase I was targeting. I figured the majority of my userbase would be more familiar with dotnet than npm, and this solidified the decision. However, whatever platform you choose I believe theres enough packages available to make a great CLI experience.

# How to create a .NET CLI tool

This guide shows you how to create a bare-bones .NET CLI tool.

### 1. Creating a new project

Let's start by creating a new directory and console application.

1. `mkdir barebonescli`
2. `cd barebonescli`
3. `dotnet new console`
4. The console app will already come with a standard-issue `Console.Writeline("Hello World");` ðŸ˜

### 2. Package as a tool

What distinguishes your console app from a dotnet tool is purly the .csproj file. In particular the package as a tool option `<PackAsTool>true</PackAsTool>`, Id which needs to be unique across all the NuGet packages in the gallery `<Id>azlazy</Id>` and tool command name `<ToolCommandName>azlazy</ToolCommandName>`. The full .csproj should look like this,

```
<Project Sdk="Microsoft.NET.Sdk">
  <PropertyGroup>
    <OutputType>Exe</OutputType>
    <TargetFramework>netcoreapp3.1</TargetFramework>
    <RootNamespace>barebonescli</RootNamespace>
    <PackAsTool>true</PackAsTool>
    <ToolCommandName>barebones</ToolCommandName>
    <PackageOutputPath>./nupkg</PackageOutputPath>
    <Version>1.0.0</Version>
    <Id>barebonescli</Id>
    <Authors>Faesel Saeed</Authors>
    <Owners>Faesel Saeed</Owners>
    <Title>Demo app to show a bare bones CLI</Title>
    <Description>This great CLI can greet the world</Description>
    <Copyright>Copyright 2020 Faesel Saeed</Copyright>
    <PackageRequireLicenseAcceptance>false</PackageRequireLicenseAcceptance>
    <PackageLicenseFile>LICENSE.txt</PackageLicenseFile>
    <PackageIconUrl>https://raw.githubusercontent.com/faesel/barebonescli/main/barebones/icon.png</PackageIconUrl>
    <PackageTags>barebones greeting cli</PackageTags>
    <RepositoryUrl>https://github.com/faesel/barebonescli.git</RepositoryUrl>
    <RepositoryType>git</RepositoryType>
    <RepositoryBranch>main</RepositoryBranch>
    <PackageProjectUrl>https://github.com/faesel/barebonescli</PackageProjectUrl>
    <PackageIcon>icon.png</PackageIcon>
  </PropertyGroup>
  <ItemGroup>
    <PackageReference Include="Microsoft.Extensions.DependencyInjection" Version="3.1.9" />
    ...
    <None Include="LICENSE.txt" Pack="true" PackagePath="$(PackageLicenseFile)" />
    <None Include="icon.png" Pack="true" PackagePath="\" />
  </ItemGroup>
</Project>
```

*The Github links are fictional, you can replace them with your project.*

To make your package more credible in the NuGet gallery there are other fields you can fill in. For the licence and package icon, you will need to place the files in the same project, the folder structure will look something like this, 

barebonescli
- barebonescli.csproj
- Program.cs
- icon.png
- LICENCE.txt

### 3. Package your CLI

The next step is to create a NuGet package, if you specified a `PackageOutputPath` in your .csproj you will see the NuGet package in that folder. 

1. Run `dotnet pack` on the project

Once executed you should have a `barebonescli.1.0.0.nupkg` file.

### 4. Upload your package to the [Nuget gallery](https://www.nuget.org/)

1. Click on your profile
2. Select upload package and upload the nupkg file

![Nuget package upload](//images.ctfassets.net/wjg1udsw901v/4nzpdnGHTMflgXhTNRw57Y/a37a98cfc3a5bcf56e6ba1cbb7bbb5ac/uploadingpackages.png)

You could also alternatively use the CLI to push with `nuget push barebonescli.1.0.0.nupkg`

Once uploaded it will be ready to install as soon as its indexed,  `dotnet tool install --global barebonescli --version 1.0.0` ðŸ˜Ž

From here the packages mentioned in this article can help you create a professional-looking CLI.

# More about Az-Lazy

If you're interested in checking out Az-Lazy, you can install in with the command,

`dotnet tool install --global az-lazy`

# Usefull links

Do checkout *Nuget Must Haves* command-line tagged package list, there's some great options in there that are not mentioned in this article.

[Nuet Must Haves - CommandLine Packages](https://nugetmusthaves.com/Tag/Commandline?page=1)
18:T3d71,As an engineer, I have always had a heavy reliance on REST'ful API's for passing information between applications. With the introduction of [open API specification](https://swagger.io/specification/) now in version 3.0.3, integration has never been easier. The push to break monoliths into microservices has further boosted its usage, however I always found one size never fits all.

## Where REST falls down ðŸ‘Ž

RESTful services have many shortfalls built in, if you're in my boat and most of the time your creating services and working on client applications. Having to tailor a client library to call those services has always been a tiresome task. 3rd party tooling like [Nswag](https://github.com/RicoSuter/NSwag) made some attempt to fix this problem however I still find breaking changes between versions that make huge changesets across all your endpoints. If your working across multiple languages like C# and Javascript your work doubles up. 

There are also encumbrances experienced when mixing batch/bulk operations, overnight jobs with REST'ful APIs. Leading to complex solutions that auto scale or spread load over time. Having to go through each request-response cycle on bulk is just in-efficient.

In most cases, responses are also in the form of JSON which is designed to cater for human readability at the expense of being inefficient. If you talking machine to machine readability is not a concern?  

Lets also not mention those endless subjective PR threads trying to decide whats RESTful and whats not ðŸ˜‡.

## Can GRPC fill the gaps ðŸ¤·â€â™‚ï¸

If you experienced the REST'ful pains above, GRPC's got your back. To get a quick demonstration of its capabilities, I recommend [Shawn Wildermuth's gRPC Talk at netPonto User Group (2020)](http://wildermuth.com/2020/07/09/gRPC-Talk-at-netPonto) he explains it in a easy to understand way.

https://www.youtube.com/watch?v=3wUtQb6C7to

To sum up its capabilities it has two key differences to REST (if your already familiar with this, skip to section **Things to look into**).

### 1. Proto files

Proto files contain the definition of your API in a structured spec compliant way. The code below shows a simple *GreetingsService* with a basic request and response.

```
syntax = "proto3";

option csharp_namespace = "HelloService";

service GreetingsService {
    rpc GetHello (HelloRequest) returns (HelloResponse);
}

message HelloRequest {
    int32 HelloCount = 1;
}

message HelloResponse {
    string HelloDescription = 1;
}
```

Proto files can then be used to transpile code [into many languages](https://grpc.io/docs/languages/). When transpiling we have the option to either create code for a **server** or **client**. Code generation creates a base class **GreetingsServiceBase** for us (it's generated in the bin folder on build time). Eventually, you end up with a service that looks like this:

```
using Grpc.Core;
using HelloServer;
using System.Threading.Tasks;

namespace TaxServer.Services
{
    public class HelloGrpcService : GreetingsService.GreetingsServiceBase
    {
        public override async Task<HelloResponse> GetHello(HelloRequest request, ServerCallContext context)
        {
            return new HelloResponse { HelloDescription = $"{request.HelloCount} Hellos to you!" };
        }
    }
}
```

The act of sharing and distributing proto files means that consuming clients can easily create their own client code and be completely agnostic of language.

### 2. Defining request/response lifecycle

GRPC allows you to change its request/response lifecycle, it has 4 options described below,

- **Unary RPC's**: Unary RPCs where the client sends a single request to the server and gets a single response back.
- **Server Streaming RPC's**: Server streaming RPCs where the client sends a request to the server and gets a stream to read a sequence of messages back.
- **Client Streaming RPC's**: Client streaming RPCs where the client writes a sequence of messages and sends them to the server, again using a provided stream.
- **Bi-Directional Streaming RPC's**: Bidirectional streaming RPCs where both sides send a sequence of messages using a read-write stream.

[*Taken from GRPC.io*](https://grpc.io/docs/what-is-grpc/core-concepts/)

These additional modes are more suited for batch processing over your traditional request/response lifecycle.

## Things to look into âœ…

So far so great, getting to this point is relatively easy and straightforward. Problem is all the tutorials seem to end at this point ðŸ˜Ÿ. To have a live API several additional concerns need to be addressed. My list was as follows:

1. Check how we can consume/distribute .proto files
2. How to create a health checking probe for a GRPC service
3. How to version endpoints
4. Can a .NET Framework client app consume a .NET Core GRPC server?
5. How to debug with tools, call an endpoint
6. Authentication and authorization
7. Can you call the service from a browser?

### 1. Check how we can consume/distribute .proto files

There are two different approaches to achieve this, mainly dependent on whether your service is internal or external public facing.

#### Option 1 - With nuget packages

Option one is to distribute your proto files using Nuget packages. This solution is recommended in the situation where you are using GRPC for internal services. Your solution structure would look something like this:

- HelloService.Protos
      - Hello.protos
- HelloService.Server
      - Server code ... 

In this case we would use a Nuspec file to package the .protos and output it into the following structure in the client app. Considering you could be consuming more than one GRPC service it might make sense to create subfolders to know where the proto file comes from.

- HelloClient
      /Protos/**service name goes gere**/Hello.protos

From here the client application can generate its client service code using the protofile. If you want to go one step further there is a dotnet command you can use to integrate the proto file into the **.csproj** file using a [dotnet command](https://docs.microsoft.com/en-us/aspnet/core/grpc/dotnet-grpc?view=aspnetcore-3.1) which can be triggered after the installation of the package.

```
dotnet grpc add-file Hello.proto
```

#### Option 2 - With a discovery endpoint

This approach is recommended if your GRPC service is a service meant for external consumers. The idea behind this approach is to expose which services/endpoints are available. The method is dependent on the [**Grpc.Reflection**](https://www.nuget.org/packages/Grpc.Reflection/) Nuget package.

The general approach is outlined [here](https://github.com/grpc/grpc/blob/master/doc/csharp/server_reflection.md). 

Once implemented it allows you to use an endpoint from the server code to generate your client code. Dotnet has a [GRPC CLI tool](https://www.nuget.org/packages/dotnet-grpc-cli/), that can read from a server reflection endpoint and produce a proto file out of it. The command looks like this,

```
dotnet grpc-cli dump https://localhost:5001 Reflection.HelloService
```

You can also write the proto file to disk using this command

```
dotnet grpc-cli dump http://localhost:10042 Reflection.HelloService -o ./prot
```

### 2. How to create a health checking probe for a GRPC service

Health checking probe endpoints are useful for monitoring uptime as well as managing containers when services are unresponsive. GRPC specification has a defined structure for creating your health checking endpoint called the [**GRPC Health Checking Protocol**](https://github.com/grpc/grpc/blob/master/doc/health-checking.md). 

However, since we are using asp.net core we can get away from this and rely on middleware to do this for us with little code.

```
public void ConfigureServices(IServiceCollection services)
{
  services.AddGrpc();
  services.AddHealthChecks();
  ...
}

public void Configure(IApplicationBuilder app)
{
  app.UseEndpoints(endpoints =>
  {
    endpoints.MapHealthChecks("/healthz");
    ...
  });
}
```

Now when running locally **https://localhost:5001/healthz** we can get a 200 response. Here is what the output logs look like:

```
      Request starting HTTP/2 GET https://localhost:5001/healthz
info: Microsoft.AspNetCore.Routing.EndpointMiddleware[0]
      Executing endpoint 'Health checks'
info: Microsoft.AspNetCore.Routing.EndpointMiddleware[1]
      Executed endpoint 'Health checks'
info: Microsoft.AspNetCore.Hosting.Diagnostics[2]
      Request finished in 19.056ms 200 text/plain
```

### 3. How to version endpoints

The problem of versioning is easily solved using namespaces, it's just a case of incorporating your version number into the namespace like so,

```
option csharp_namespace = "HelloService.v1";
```

For each version, you would have different proto files and different service implementations. When inheriting from the base we can be specific on the version we need.

**Server Code**

```
public class HelloGrpcService : HelloService.v1.GreetingsService.GreetingsServiceBase
{
}
```

**Client Code**

The namespaces segregate the types so it just works out.

```
//Version 1
using var channel = GrpcChannel.ForAddress("https://localhost:5001");
var client = new HelloService.v1.GreetingsService.GreetingsServiceClient(channel);
var response = await client.GetHello(new HelloService.v1.HelloRequest() { 
      HelloCount = 1
});

//Version 2
using var channel2 = GrpcChannel.ForAddress("https://localhost:5001");
var client2 = new HelloService.v2.GreetingsService.GreetingsServiceClient(channel);
var response2 = await client2.GetHello(new HelloService.v2.HelloRequest() {
      HelloCount = 2
});
```

### 4. Can a .NET Framework client app consume a .NET Core GRPC server?

Turns out it can yes, however ... as GRPC is built upon HTTP/2 which is not supported in .net framework, making secure connections to your API is not possible. The client code for .net framework is very similar, we just pass a **ChannelCredentials.Insecure** option in when building the client.

```
var channel = new Channel("127.0.0.1", 5000, ChannelCredentials.Insecure);
var client = new GreetingsService.GreetingsServiceClient(channel);
```

### 5. How to debug with tools, call an endpoint

If you're like me and you've come from a REST background your most likely used to polished tools like Postman or Insomnia to test out your endpoints. Sadly these tools don't support GRPC ðŸ˜¢... yet anyway...

The [GRPC Tooling Community](https://github.com/grpc-ecosystem/awesome-grpc#lang-cs) is still in its infancy. There are however some new players that are emerging that get the job done, most notably for me BloomRPC. 

![BloomRPC](//images.ctfassets.net/wjg1udsw901v/6p6bGkCU7Tpa3tMwPsoRxO/adb23285ed92ea9c2c757ef62a4600a9/BloomRPC.png)

After importing in your proto files you get a great swagger-esk UI that automatically build up your request body from your proto file.

### 6. Authentication and authorization

Because we are working under the guise of asp.net core we can take advantage of its authentication middleware. The following authentication methods are supported.

- Azure Active Directory
- Client Certificate
- IdentityServer
- JWT Token
- OAuth 2.0
- OpenID Connect
- WS-Federation

Below is a simple code example of authenticating a JWT token with an identity service. As you can see its no different from a REST service.

```
public void ConfigureServices(IServiceCollection services)
{
    var authority = "https://myidentityserver.com";

    services
        .AddAuthentication("Bearer")
        .AddJwtBearer("Bearer", options =>
        {
            options.Authority = authority;
            options.RequireHttpsMetadata = false;
            options.TokenValidationParameters = new TokenValidationParameters
            {
                ValidateAudience = false,
            };
            options.ConfigurationManager = new ConfigurationManager<OpenIdConnectConfiguration>
            (
                metadataAddress: authority + "/.well-known/openid-configuration",
                configRetriever: new OpenIdConnectConfigurationRetriever(),
                docRetriever: new HttpDocumentRetriever { RequireHttps = false }
            );
            options.Events = new JwtBearerEvents
            {
                OnTokenValidated = context =>
                {
                    var ci = (ClaimsIdentity)context.Principal.Identity;
                    var authHeader = context.Request.Headers["Authorization"];
                    var token = authHeader.FirstOrDefault()?.Substring(7);
                    if (token != null)
                    {
                        ci.AddClaim(new Claim("token", token));
                    }

                    return Task.CompletedTask;
                }
            };
        });

    services.AddAuthorization();
    ...
}

public void Configure(IApplicationBuilder app)
{
    app.UseAuthentication();
    app.UseAuthorization();
    ...
}
```

Below is output from an authenticated request:

```
      Request starting HTTP/2 POST https://localhost:5001/HelloGrpcService.GreetingsService/GetHello application/grpc
info: Microsoft.AspNetCore.Authentication.JwtBearer.JwtBearerHandler[2]
      Successfully validated the token.
info: Microsoft.AspNetCore.Authorization.DefaultAuthorizationService[1]
      Authorization was successful.
info: Microsoft.AspNetCore.Routing.EndpointMiddleware[0]
      Executing endpoint 'gRPC - /HelloGrpcService.GreetingsService/GetHello'
Request parameter 1
Request came from test-client-id
info: Microsoft.AspNetCore.Routing.EndpointMiddleware[1]
      Executed endpoint 'gRPC - /HelloGrpcService.GreetingsService/GetHello'
info: Microsoft.AspNetCore.Hosting.Diagnostics[2]
      Request finished in 5865.2411ms 200 application/grpc
```

### 7. Can you call the service from a browser?

Currently, as it stands the answer is no, browsers don't offer fine-grained control over API requests to support GRPC. However, there is some light at the end of the tunnel.

Back in 2016 Google started working on a specification for "GRPC for the browser". You can read more about it [here](https://grpc.io/blog/state-of-grpc-web/) but in essence,

>The basic idea is to have the browser send normal HTTP requests (with Fetch or XHR) and have a small proxy in front of the gRPC server to translate the requests and responses to something the browser can use - grpc.io

In the C# world, Microsoft has an implementation of this specification in their docs, [Use gRPC in browser apps](https://docs.microsoft.com/en-gb/aspnet/core/grpc/browser?view=aspnetcore-3.1).

There are some disclaimers to this, as gRPC supports streaming and bidirectional requests this addition is only recommended for unary requests. Due to this limiting factor helpers are present to turn it on and off for services when setting up GRPC services in the startup,

```
endpoints.MapGrpcService<HelloGrpcService>().EnableGrpcWeb().RequireCors("AllowAll");
```

What I find particularly interesting is that the problem grpc-web solves is similar to the problems we have with .net framework (https/2 is not supported). Could this perhaps be an answer to getting secure requests working? ... sadly not yet! at the moment its not possible as grpc-web was was built on .net standard 2.1 so .net framework is not supported. Perhaps there might be movement on this in time to come.

## Things I missed out

1. Integration Testing, im a big fan of using in memory testing with Test Server it would be interesting to see if this works with a GRPC service.

## Useful Links

1. [C# Examples](https://github.com/grpc/grpc/tree/master/src/csharp)
2. [More c# examples](https://github.com/grpc/grpc-dotnet/tree/master/examples)
19:T1e32,As a windows user the terminal experience has always been lacking, up till the new [windows terminal](https://www.microsoft.com/en-gb/p/windows-terminal/9n0dx20hk701?rtc=1&activetab=pivot:overviewtab) was released. Incorporating [WSL (Windows Subsystem for Lynx)](https://docs.microsoft.com/en-us/windows/wsl/install-win10) really helped bridge that gap as it opens up console experience that makes use of **apt-get** use the plethora of packages available. 

Having tried using this for react apps I found the experience really slow when building apps. Running the same app in Powershell would start up in a fraction of the time. This got me thinking..

> Can I evolve my Powershell console experience in the same way I can with WSL?

Looking at [Powershells Gallery](https://www.powershellgallery.com/), they have a total of **7,091** unique packages. There must be some things here we can use.

This article is about my journey from reading this blog post ["How to make a pretty prompt in Windows Terminal with Powerline, Nerd Fonts, Cascadia Code, WSL, and oh-my-posh"](https://www.hanselman.com/blog/HowToMakeAPrettyPromptInWindowsTerminalWithPowerlineNerdFontsCascadiaCodeWSLAndOhmyposh.aspx) by Scott Hanselman, to taking the experience one step further to evolve my terminal. Video on it here:

https://www.youtube.com/watch?v=lu__oGZVT98

## Prerequisites

This article assumes you have already read Hanselman's article and kitted your terminal out with the following (if not do so):

- Added Cascadia Ligature Font, for more compact and expressive text.
- Added oh-my-posh, for better git branch information.

## Making use of your Powershell profile

Before your Powershell window starts, any code in your Powershell profile is executed first. You can find your profile directory location by typing the following in your console,

```
$PROFILE
```

It's from this file we can begin importing in scripts and adding custom functions. If you followed the prerequisites, your profile file will be mostly empty except perhaps a command to set the theme in oh-my-posh.

```
Set-Theme Paradox
```

## Incorporating packages

Before we start importing packages we need to get **PowerShellGet** setup, installation steps can be found here [Installing PowerShellGet](https://docs.microsoft.com/en-gb/powershell/scripting/gallery/installing-psget?view=powershell-7). Once complete you can start installing scripts like so,

```
Install-Module -Name WifiTools 
```

That's it, you can actually start checking your wifi signal with `Show-WifiState` ðŸ“¶! Aside from wifi tools here are some choice selections, some of these are recommended as it helps create building blocks for your own custom scripts:

### [1. Terminal Icons](https://www.powershellgallery.com/packages/Terminal-Icons/0.1.1)

[Terminal-Icons](https://www.powershellgallery.com/packages/Terminal-Icons/0.1.1) really helps improve visibility when navigating. It also allows you to format your directory list wide so that you can see all the files and folders without scrolling down.

You can activate it in your Powershell profile by adding the following import

```
Import-Module -Name Terminal-Icons
```

And that's it! witness the beauty:

![Terminal Icons](//images.ctfassets.net/wjg1udsw901v/5GsSsHw0n1zQ2ST9ItYIM6/1f8e2a1d08542fdfaa24acc1b35dbc02/terminal-icons.png)

### [2. Tree](https://www.powershellgallery.com/packages/Tree/1.0.1)

[Tree](https://www.powershellgallery.com/packages/Tree/1.0.1) helps with directory discover and searching by printing in (as the name suggests) a tree structure. I alias this package and mainly use it for listing and searching, below is what you need in your Powershell profile,

```
Install-Module -Name Tree 
```
```
# Shows a tree structure of the current directory (excluding folders you want to ignore)
function treels {
    Get-ChildItemTree . -I 'node_modules|bin|obj|.git|.vs'
}

# Searches the current directory for a pattern (wildcards * are accepted) and returns the tree view with matching files
function treef ([string] $pattern) {
    Get-ChildItemTree . -P $pattern -I 'node_modules|bin|obj|.git|.vs'
}
```

Example showing a directory:

![Tree Directory List](//images.ctfassets.net/wjg1udsw901v/2TiqouIH6bTQ4U6quojGO7/0c1d294bac92233dd453300b482afb93/TreeList.png)

Example search for a file:

![Tree Search](//images.ctfassets.net/wjg1udsw901v/p7NOrYHf5oRdLz9IqwDzR/7de6b06b6233dc5d308dead818926593/TreeSearch.png)

### [3. Burnt Toast](https://www.powershellgallery.com/packages/BurntToast/0.7.1) 

[BurntToast](https://www.powershellgallery.com/packages/BurntToast/0.7.1) great package to notify yourself of any long running tasks. It hooks into the native windows toast notification system and has a plethora of options. One ideal use for this is cloning a repo (I'm talking about that repo dating back to the dinosaurs ðŸ‰) since it lets you kick it off and automatically get a notification when done. 

```
Install-Module -Name BurntToast
```
```
# clones a repo and notify with a toast
function clonem([string] $url) {
    git clone $url
    New-BurntToastNotification -AppLogo 'C:\Icons\completed.png' -Text "Finished!", 'Finished cloning repo'
}
```

I also find it great for scheduling in reminders, this one is dependent on a function which I import at the top of the profile [code example found here](https://github.com/Windos/BurntToast/blob/master/Examples/Example05/New-ToastReminder.ps1).

```
# IMPORT CUSTOM FILES
. "C:\Users\faese\Documents\WindowsPowerShell\Custom\BurntToast.ps1"

# trigger a remind after x minuites with some custom text
function reminder([int]$minuites, [string]$text) {
    New-ToastReminder -AppLogo 'C:\Icons\reminder.png' -Minutes $minuites -ReminderTitle 'Reminder Reminder!' -ReminderText $text
}
```

Example to ensure you never eggless,

![Toast Reminder](//images.ctfassets.net/wjg1udsw901v/4zg5SUqQ14zjuZE6QArrj0/6f2809c3942708f89aea47dd3c815bdb/reminder.png)

### [4. ColoredText](https://www.powershellgallery.com/packages/ColoredText/1.0.6)

This library essentially allows you to print in different colours pure and simple. I mainly use this as a confirmation line when chaining together several commands so your eye just looks for coloured text to look for completion.

```
Install-Module -Name ColoredText
```
```
# create a new branch
function newb([string]$branchName){
    git branch $branchName
    git checkout $branchName --track

    $message = "Finished creating branch: " + $branchName

    cprint black $message on rainbow print
}

# publish branch
function publish {
    $branchName = git rev-parse --abbrev-ref HEAD
    git push --set-upstream origin $branchName

    $message = "Finished publishing branch: " + $branchName

    cprint black $message on rainbow print
}
```

![Colored Text](//images.ctfassets.net/wjg1udsw901v/6IctC3h57SGaYFmQelmlx4/2840d80d68895fe0dde359b64328c345/coloredtext.png)

### Some other honourable mentions

Below are some other honourable mentions that I needed more time to investigate,

- [PS Menu](https://www.powershellgallery.com/packages/ps-menu/1.0.6), allows you to create a multi-select menu of options.
- [WTToolBox](https://www.powershellgallery.com/packages/WTToolBox/1.6.0), helps manage your windows terminal, I was mainly going to use this to get a list of shortcuts.

## Download my profile code here

You can download my complete profile code [here](https://github.com/faesel/terminal-profile)

## Summary

I'm still on a path of discovery with Powershell, I've seen some great packages that can help make my terminal experience fast and efficient. However, I do feel I'm at the very beginning of this journey and with the continued improvements to WSL, I may flip flop to the Linux side.1a:T36b9,I recently recreated my blog in GatsbyJs, you can download a template of it here [gatsby-techblog-starter](https://github.com/faesel/gatsby-techblog-starter). In the joy of sharing its simplicity to the world, I tweet about my [intro article](https://www.faesel.com/blog/gatsby-tech-blog-starter) with a link to my website. To my dismay, I noticed the tweet was lacking a lot of formatting and information on the link... would you even see that link ðŸ‘€?

![Twitter without opengraph](//images.ctfassets.net/wjg1udsw901v/37l0W6HYcNXrTDrAbMDJSn/04d9b245622621d5b0eafaf59657ea07/twitter-no-opengraph.png)

I realised the secret sauce I was missing was called **Open Graph Protocol**. From the [specifications website](https://ogp.me/) itself, 

> The Open Graph protocol enables any web page to become a rich object in a social graph. For instance, this is used on Facebook to allow any web page to have the same functionality as any other object on Facebook.

In essence its the mata tags you see below, that sites like Twitter, Linked In, Facebook use to correctly render an enriched link of the page on their website,

```javascript
<meta data-react-helmet="true" name="twitter:card" content="summary_large_image">
<meta data-react-helmet="true" name="twitter:site" contact="@faeselsaeed">
<meta name="twitter:creator" content="" data-react-helmet="true">
<meta name="twitter:title" content="Creating my dream tech blog with GatsbyJS" data-react-helmet="true">
<meta name="twitter:description" content="I'm someone who's always had my own tech blog, Iâ€¦" data-react-helmet="true">
<meta name="twitter:image" content="//images.ctfassets.net/wjg1udsw901v/6hjsGXkoyitmyiEuBdeTP2/c77e74af9235ac775f18836e2de07cac/gatsby-logo.jpg" data-react-helmet="true">

<meta property="og:site_name" content="" data-react-helmet="true">
<meta property="og:title" content="Creating my dream tech blog with GatsbyJS" data-react-helmet="true">
<meta property="og:url" content="https://www.faesel.com/blog/gatsby-tech-blog-starter" data-react-helmet="true">
<meta property="og:description" content="I'm someone who's always had my own tech blog, Iâ€¦" data-react-helmet="true">
<meta property="og:image" content="//images.ctfassets.net/wjg1udsw901v/6hjsGXkoyitmyiEuBdeTP2/c77e74af9235ac775f18836e2de07cac/gatsby-logo.jpg" data-react-helmet="true">
<meta property="og:image:alt" content="Gatsby JS" data-react-helmet="true">
<meta property="og:type" content="article" data-react-helmet="true">
```

This article is about how I used [Helmet JS](https://helmetjs.github.io/) to improve my sites shareability and improving its SEO capabilities.

## Step 1 - Install those dependencies

The dependencies we are interested in are as follows:

```
npm intall gatsby-plugin-react-helmet react-helmet
```

You can read more about the gatsby plugin [here](https://www.gatsbyjs.org/packages/gatsby-plugin-react-helmet/) along with more detailed information on Helmet js and all its supported tabs [here](https://github.com/nfl/react-helmet)

## Step 2 - Store your constant's in your gatsby config

When creating a Gatsby website we always have a config file in the root of the project called **gatsby-config.js**, from here we can add various plugins like so,

```javascript
module.exports = {
    plugins: [
        'gatsby-plugin-react-helmet'
    ]
}
```

This config file is also the place to store all you common reusable information in Gatsby's predefined **siteMetadata** tag (this tag makes it accessible through GraphQl). We will be using this later on to populate our head with various information.

```json
module.exports = {
    siteMetadata: {
        title: 'FAESEL.COM',
        author: 'Faesel Saeed',
        description: 'Personal blog of Faesel Saeed',
        siteUrl: 'https://www.faesel.com',
        social: {
            linkedin: 'https://www.linkedin.com/in/faesel-saeed-a97b1614',
            twitter: 'https://twitter.com/@faeselsaeed',
            twitterUsername: '@faeselsaeed',
            github: 'https://github.com/faesel',
            flickr: 'https://www.flickr.com/photos/faesel/',
            email: 'faesel@outlook.com'
        },
        rssFeedUrl: '/rss.xml'
    },
    ...
}
```

## Step 3 - Create your head component

Now that we have all our static information in the config we can query this out using GraphQl through the objects > **site** > **siteMetadata**. We can also import in **Helmet** and start building up our Head meta data. My Head component looks like this,

```javascript
import React from 'react'
import { Helmet } from 'react-helmet'
import { useStaticQuery, graphql } from 'gatsby'

import favicon from '../../static/favicon.ico'

const Head = ({ pageTitle, title, url, description, imageUrl, imageAlt, type datePublished }) => {
    const data = useStaticQuery(graphql`
        query {
            site {
                siteMetadata {
                    siteUrl,
                    title,
                    author,
                    social {
                        twitterUsername
                    }
                }
            }
        }
    `)

    return (
        <>
            <Helmet title={`${pageTitle} | ${data.site.siteMetadata.title}`} />
            <Helmet>
                <link rel="icon" href={favicon} />

                <meta name="twitter:card" content="summary_large_image"></meta>
                <meta name="twitter:site" contact={data.site.siteMetadata.social.twitterUsername}></meta>
                <meta name="twitter:creator" content={data.site.siteMetadata.twitterUsername}></meta>
                <meta name="twitter:title" content={title}></meta>
                <meta name="twitter:description" content={description}></meta>
                <meta name="twitter:image" content={imageUrl}></meta>

                <meta property="og:locale" content="en_GB" />
                <meta property="og:site_name" content={data.site.siteMetadata.title} />
                <meta property="og:title" content={title}></meta>
                <meta property="og:url" content={url}></meta>
                <meta property="og:description" content={description}></meta>
                <meta property="og:image" content={imageUrl}></meta>
                <meta property="og:image:alt" content={imageAlt}></meta>
                <meta property="og:type" content={type} />
            </Helmet>
        </>
    )
}

export default Head
```
(Note some of the properties get fleshed out later on in the article)

The Helmet component injects in HTML tags into the head of the HTML document. To understand what the tags represent within the Helmet component, and to see a full range of what's available use the following two links.

1. [Tags from Open Graph](https://ogp.me/)
2. [Tags from Twitter](https://developer.twitter.com/en/docs/tweets/optimize-with-cards/overview/markup)

## Step 3 - Using your head component

Using your head component is quite straight forward, its more a case of working out where to source all your properties. Here's what my page looks like,

```javascript
import React from "react"
import { graphql } from "gatsby"
import Layout from "../components/layout"
import Head from "../components/head"

// Add some code here to get all your data from markdown, cms etc.

const Blog = props => {
  return (
    <Layout>
      <Head 
        pageTitle={props.data.title}
        title={props.data.title}
        description={props.data.bodym.childMarkdownRemark.excerpt}
        url={`${props.data.site.siteMetadata.siteUrl}/blog/${props.data.slug}`}
        imageUrl={props.data.hero.file.url}
        imageAlt={props.data.hero.title} 
        type='article' 
        datePublished={props.data.contentfulBlog.iso8601DatePublished}/>

      <h1>My Great Blog Post</h1>
      ...
    </Layout>
  )
}

export default Blog

```

## Step 4 - Go further with JSON-LD and Linked data

So far so great, we have enough here for most social media sites to understand the structure of our data and to use this to correctly format the information on a consuming website. But what do search engines use?

The answer is [Json-ld and linked data](https://json-ld.org/), best explained by the specs website itself,

> JSON-LD is a lightweight Linked Data format. It is easy for humans to read and write. It is based on the already successful JSON format and provides a way to help JSON data interoperate at Web-scale. JSON-LD is an ideal data format for programming environments, REST Web services, and unstructured databases such as Apache CouchDB and MongoDB.

and,

> Linked Data empowers people that publish and use information on the Web. It is a way to create a network of standards-based, machine-readable data across Web sites. It allows an application to start at one piece of Linked Data, and follow embedded links to other pieces of Linked Data that are hosted on different sites across the Web. 

To sum it up in one sentence *we are using JSON data to create structured information so that websites can deep link with each other*. With this in mind our head component looks like this:

```javascript
import React from 'react'
import { Helmet } from 'react-helmet'
import { useStaticQuery, graphql } from 'gatsby'

const Head = ({ pageTitle, title, url, description, imageUrl, imageAlt, type, datePublished }) => {
    const data = useStaticQuery(graphql`
        query {
            site {
                siteMetadata {
                    siteUrl,
                    title,
                    author,
                    social {
                        twitterUsername
                    }
                }
            }
        }
    `)

    const ldJsonBreadcrumb = {
        '@context': 'https://schema.org',
        '@type': 'BreadcrumbList',
        'itemListElement': [{
            '@type': 'ListItem',
            'position': 1,
            'name': 'Home',
            'item': `${data.site.siteMetadata.siteUrl}/home`
        },{
            '@type': 'ListItem',
            'position': 2,
            'name': 'Blog',
            'item': `${data.site.siteMetadata.siteUrl}/blog`
        },{
            '@type': 'ListItem',
            'position': 3,
            'name': 'Projects',
            'item': `${data.site.siteMetadata.siteUrl}/projects`
        },{
            '@type': 'ListItem',
            'position': 4,
            'name': 'Contact',
            'item': `${data.site.siteMetadata.siteUrl}/contact`
        }]
      };

    const jsonldArticle = {
        '@context': 'http://schema.org',
        '@type': `${type}`,
        'description': `${description}`,
        'image': {
            '@type': 'ImageObject',
            'url': `${imageUrl}`
        },
        'mainEntityOfPage': {
            '@type': 'WebPage',
            '@id': `${data.site.siteMetadata.siteUrl}`
         },
        'inLanguage': 'en',
        'name': `${title}`,
        'headline': `${title}`,
        'url': `${url}`,
        'datePublished': `${datePublished}`,
        'dateModified': `${datePublished}`,
        'author': {
            '@type': 'Person',
            'name': `${data.site.siteMetadata.author}`
        },
        'publisher' : {
            '@type': 'Organization',
            'name': `${data.site.siteMetadata.author}`,
            'logo': {
                '@type': 'ImageObject',
                'url': `https://images.ctfassets.net/wjg1udsw901v/4RI5COhSqeYFCbvzYFeFZW/af52277ab41da56c1be5f72f316befe9/logo.png`
            }
        }
    };

    return (
        <>
            <Helmet>
                {/* other head elements go here */}
      
                <script type="application/ld+json">
                    {JSON.stringify(ldJsonBreadcrumb)}
                </script>
                
                {type === 'article' && (
                   <script type="application/ld+json">
                        {JSON.stringify(jsonldArticle)}
                    </script>
                )}
  
                {/* Meta properties go here */}
                
            </Helmet>
        </>
    )
}

export default Head
```

For more information on the structure you can read up on the [W3C Json-LD specification document](https://www.w3.org/2018/jsonld-cg-reports/json-ld/#introduction)

To get an idea of the full range of tags available take a look at these two links (in the case of my website I only use **BreadcrumbList** and **Article** types depending on what content you have you may show something else).

1. [BreadcrumbList](https://schema.org/BreadcrumbList)
2. [Article](https://schema.org/Article)

Do note for the property **datePublished** you need to format your dates in [ISO-8601](https://en.wikipedia.org/wiki/ISO_8601) format. To save you a trip in google this up you can use the GraphQl query snippet below. The format definition comes from [Moment JS](https://momentjs.com/) which Gatsby is using under the hood.

```
iso8601DatePublished: datePublished(formatString: "YYYY-MM-DD[T]HH:mm:ss.SSS[Z]")
```

## Step 5 - Validating your tags

There are actually various websites we can use to validate your tags and data. When building my website I used the following sources.

- [Twitters Card testing tool](https://cards-dev.twitter.com/validator)
- [Googles link for testing json-ld](https://search.google.com/structured-data/testing-tool/u/0/), testing and validating tool for JSON-LD. It goes as far as telling you if there are any missing tags.
- [Matatags.io](https://metatags.io/), great for previewing how you website links will render on all the social media websites

TIP ðŸ˜Ž! If your implementing this retrospectively twitter will update all your previous tweets with with extra formatting, but it does take about a week. If you want to refresh it quicker, you can use the **Twitters Card testing tool** mentioned above to clear out the cache for an individual post.

After that you can begin to tweet with confidence ðŸ˜

![Tweet Format](//images.ctfassets.net/wjg1udsw901v/2ojzLoP0kubfHc6NdpTiIz/483dd1458ab941764b34ba1e3d5e67e9/TweetFormat.png)1b:T2804,I'm someone who's always had my own tech blog, I've gone through two revisions already with the last revision updating out of a 1997 style book. How much I contribute to the blog has always depended on how much friction and effort it takes to write content, manage and upload photos and paste in code. 

My previous revisions at its core have always been dependent on using opensource wysiwyg editors. Which for me have been deficient in several ways.

1. Behind the scenes, they produce HTML which always contains ghost ðŸ‘» spaces.
2. You always have to build in your own system to upload and manage photos.
3. Pasting in code and getting it to format correctly has always been difficult.
The content I write belongs to the actual website, so every time I change or rebuild my website I have to migrate all my content across.

With all this in mind, I wanted a solution that leverages a content management system that supports markdown. Is free and open source, and easily extendable using a modern JavaScript framework. The combination I chose contains the following,

- **Gatsby** Framework built on React that creates a really fast experience and is easily extendable.
- **Contentful** One of the leading content management systems that support markdown.
- **Disqus** One of the most popular commenting systems mainly chose this as a continuation from my previous website.
- **Github Pages** Free hosting by the GitHub guys.

Due to everything being either free or opensource, you can download the source code [here](https://github.com/faesel/gatsby-techblog-starter), follow the steps in the readme or below in this article and get started with your own tech blog.

## Why use Contentful? 

If your someone like me and you don't want to be restricted to the world of WordPress websites, Contentful is the next best thing. As the name describes its a repository for your content and nothing else. Where Contentful really shines is in how easy it is to get your content out. Everything is API accessible and in a JSON format, so if I ever decide to build v4 its just a case of hooking up the CMS.

Contentful also supports writing in markdown format which is absolutely crucial if you're going to be pasting in code (as markdown supports syntax highlighting you will get the most accurate code colours when rendering).

![Contentful UI](//images.ctfassets.net/wjg1udsw901v/107DfGPIMth9cXJcd7aXNf/73b9fa04c61fbb34a5f33efda16e82d4/contentful.png)

## Why use Disqus?

As mentioned Disqus was mainly a continuation from the previous version of my website, there are however some plus points to using Disqus. Namely the popularity (See image below), Disqus is by far the most popular commenting system out there, its also incredibly easy to get set up.

![Disqus vs Commento](//images.ctfassets.net/wjg1udsw901v/4y1BkGbHeRGvzMNEX6D0lO/d92bb715335b7119b10d8352da2e1c5e/disqus-vs-commento.png)

There are some annoyances however, namely the lack of markdown support, which could be an issue if a commenter wants to post a code snippet.

Perhaps at some point I will migrate to using [Commento](https://commento.io/), theres a great article comparing the two [here](https://victorzhou.com/blog/replacing-disqus/).

## Why GitHub pages?

Firstly it's free hosting! What more do you want...

I have always relied on using Azure to host my pages but unless you got a Biztalk account there's a cost in doing so. Github allows you to host content on their website for absolutely free, its got two forms of public pages. 

1. Personal pages

This sits under your personal repository path, so in my case my username is faesel (https://github.com/faesel/), when creating a repository called 'faesel' and enabling pages. GitHub gives you the domain https://faesel.github.io. What's amazing is (provided your DNS is setup correctly) they also handle SSL certificates for your behalf.

2. Project pages

Github also supports project pages which sit under a subdomain of your repository, for example a repository with the link https://github.com/faesel/faesel-blog would resolve into public page URL for https://faesel.github.io/faesel-blog.

For my blog option 1 was chosen. There is one caveat to personal pages, your published output needs to reside in master and your actual source code needs to be in another branch. The Gatsby template I have created, uses master and source as the two main branches as you can see [here](https://github.com/faesel/faesel.github.io/branches).

## Why use Gatsby?

Gatsby lightning fast when it comes to rendering pages, behind the scenes it builds up static pages which you deploy to your website. As it's designed to be a progressive web app, pre-fetching is built it, so when your cycles through different pages on your website the load times are instant.

Gatsby supports a wide range of plugins, **2162** at the time of writing this article. Admittedly some of them are duplicates, but overall they have enough coverage to integrate the vast majority of 3rd party platforms. Below are some plugins I used for my blog:

- **gatsby-plugin-feed** Used for generating an RSS feed
- **gatsby-plugin-sitemap** Used for generating a sitemap xml file
- **gatsby-plugin-gtag** Used for integrating google analytics into your blog
- **gatsby-plugin-react-helmet** SEO plugin used for setting titles and metadata
- **gatsby-plugin-sass** Integrating sass
- **gatsby-remark-highlight-code** Used for highlighting code syntax
- **gatsby-source-contentful** Used for getting data from Contentful
- **gatsby-transformer-remark** Used for transforming markdown into HTML

A full list of their plugins can be found [here](https://www.gatsbyjs.org/plugins/)

As Gatsby is built on top of React which means extending your website's functionality is easy. In addition to this data access is all powered through GraphQL which really helps tailor the requests for data in accordance with your UI. All this is setup for you right out the box.

## How to use the blog template to create my own tech blog?

As mentioned I have created a Gatsby blog template that you can clone yourself, setup and host all at no cost at all. The code can be found [here](https://github.com/faesel/gatsby-techblog-starter).

There are some pre-requisites we need to get through before we can begin, mainly creating accounts in the following 3rd partys.

- [Contentful](https://www.contentful.com/sign-up/)
- [Google Analytics](https://analytics.google.com/analytics/web/)
- [Disqus](https://disqus.com/)

### Step 1 - Configuring Contentful

The first step is to configure Contentful by creating a content model. A content model represents all the building blocks required to represent a single blog post. Below is a screenshot of the content model needed:

![Gatsby blog content model](//images.ctfassets.net/wjg1udsw901v/leOFhubn56i1CiN7MeIjK/442336bb18392148c6ac4e16ed496375/content-model.png)

If your creating it manually do remember to set the field 'BodyM' as a markdown field. Once this model is in place, you can begin writing up your first markdown post. To create this model programatically you can run the following command:

```
npm run setup SPACE_ID CONTENTFUL_MANAGEMENT_TOKEN
```

The management token can be sourced from **Settings** > **API Keys** > **Content management tokens**

There are two settings we need to take a note of that are needed for **Step 2**

- Space Id
- Space Access Token

Both of these can be sourced from **Settings** > **API Keys** > **Content delivery / preview tokens**, 

### Step 2 - Configuring Environment Variables

The next step is to populate your environment variables, the two Contentful keys can be accessed by following Step 1. Setting up google and Discus is optional.

```
CONTENTFUL_SPACE_ID=
CONTENTFUL_ACCESS_TOKEN=
GOOGLE_TRACKING_ID=
GATSBY_DISQUS_NAME=
```

### Step 3 - Configuring your gatsby config

The gatsby config file is at the root of this project, it contains all the plugins installed into this project.

```
module.exports = {
    siteMetadata: {
        title: 'FAESEL.COM',
        author: 'Faesel Saeed',
        description: 'Welcome to my great blog',
        siteUrl: 'https://www.faesel.com', //Use the fully qualified url
        social: {
            twitter: 'https://twitter.com/@faeselsaeed', //Use the fully qualified url
            linkedin: 'https://www.linkedin.com/....',
            github: 'https://github.com/....',
            flickr: 'https://www.flickr.com/....', //Feel free to remove this :)
            email: 'someone@gmail.com'
        },
    },
}
```

Once This is filled in your all set to run the project!

### Step 4 - Running the project

Begin by installing dependencies: 

**npm install**

Followed by running the website:

**npm run develop**

## Step 5 - Deployment

To deploy the project begin by creating a repository containing your GitHub username "faesel.github.io".

Copy all your code into a branch called **source**

Run the following command

**npm run publish**

The command will publish all the static files Gatsby generates into the **master** branch.

To enable github pages navigate to the **Repository** > **Settings** > Scroll down to github pages and select the source branch as **master**. Also at this step if you can enter in your custom domain. Once setup it should look something like this:

![Github Pages](//images.ctfassets.net/wjg1udsw901v/1Yc9sjRgkxBBCaHx8svVIq/5f94cf13095287375f58e96648ee799f/github_pages.png)

## Summary

Its as easy as that, you know have a blog whos content is powered by Contentful! ... time to blog.

![Gatsby blog template](//images.ctfassets.net/wjg1udsw901v/5jtQ1A9KGisfOeDCLQqPdT/49d682dbf90be3495ca0c6ad31ab85e1/website.png)

As with all things in tech, there are some improvements that got taken off the bucket list.

- Automate deployments by making use of webhooks triggered when publishing content.
- Add a plugin to allow embeding content like tweets, youtube posts ect.
- Add a searching mechanism
- Add pagination

## The Honourable mention

This article and the project was inspired by Andrew Mead's [**'The Great Gatsby Bootcamp'**](https://www.youtube.com/watch?v=kzWIUX3CpuI) course which I highly recommend in learning the basics.

https://www.youtube.com/watch?v=kzWIUX3CpuI1c:T2606,I'm currently working at a place were we are using queue triggered Webjobs to handle the sending of messages like email and SMS (using Send Grid and Twilio). Using a queue based system for this is great because it allows us to replay any queue messages, should one of the 3rd party's (or our code) fail to send the message. 

Since we are connecting into 3rd party's you can almost guarantee there's going to be some form of failure. So its always good practice to leverage on this type of architecture to handle the unknown. We have the following setup:

- WebsiteÂ > Storage QueueÂ > Web JobÂ > Send Grid
- WebsiteÂ > Storage QueueÂ > Web JobÂ > Twillio

When a failure occurs, queue messages are automatically moved from the 
message queue into a poison queue,Â these queuesÂ are always suffixed with "poison" (MS really wanted to highlight how toxic your problems are) like so:

- email - For normal operation
- email-poison - Messages moved here when a failure occurs
- sms
- sms-poison

Gaining visibility of what's in a poison queue is really important in knowing 
the health of your system. So I embarked upon a task in seeking out an 
alert setting buried deep somewhere inÂ the Azure portal to helpÂ surface 
any messages going into the poison queue. I knew this would be a metric 
alert of some kind either in the 'Storage Account', 'Alerts' or perhaps 
even 'Application Insights' blade. 

After having spent a while searching for it as well as posting this Stack OverflowÂ question (it wasn't a popular one..), I started doubting whether it even existed!

I even tried the search box at the top of the azure dashboard as a last ditch effort, hoping it will provide answers. You think this would 
exists somewhere (if it does and my eyes have deceived me please do get 
in touch) or at the very least be visible and easily findable? Alas this was not the case..

So I decided to do something about it, 

> why not have an Azure function that takes a storage account and looks through all the queues to check if any poison queue messages exist. 

Whilst were at it we could also check if messages are stacking up in the non-poison queues (just in-case a Webjob has been turned off or cant process a certain message), and even provide the content of a problematic queue message. Since our team uses slack for communication I decided to send the notification to Slack. Below are the steps I took:

#Step 1 - Setting up slack

Setting up slack is quick and easy, just create a 'poison-queue' channel, and create a new integration in the custom integrations section (Note your gonna have to get admin access to do this (I have provided a link at the bottom of this article as its nested deep in their UI). An integration is essentially a web hook endpoint for us to post JSON data to (I have added a link for Slacks JSON format below too, as well as a message builder to help customise the look and feel).

The picture below show where you can get your web hook URL from.

![Azure Queue Notifier - Slack Integration](//images.ctfassets.net/wjg1udsw901v/1y3YcZQRnN6yhsilboTyhL/5c9fad66a9601cc851c9377d26198258/slackwebhook.jpg)

#Step 2 - Create your Azure Function

Since this is not a tutorial on Azure Functions, I'm going to skip going into detail here. Microsoft however have provided some great documentation on this (with pictures!) to help you out. Links are at the end until MS break them. By the way your gonna need a cron expression to define the timeframe for this function to work in, if you hate cron as much as I do worry not! Use my cron expression for a daily sobering alert at 9:00 - 0 0 9 * * *

#Step 3 - Create your slack message structure

Next we can create the basic structure needed for our Slack message, expressed as a C# class. My class is actually quite simple and missing quite a few properties, to get a sense of all the customisations Slack offers have a look at the links below.

```csharp
#r "Newtonsoft.Json"

#load "Attachments.csx"

using Newtonsoft.Json;
using System.Collections.Generic;

public sealed class SlackMessage
{
    public SlackMessage()
    {
        Attachments = new List<Attachments>();
    }

    [JsonProperty("channel")]
    public string Channel { get; set; }

    [JsonProperty("username")]
    public string UserName { get; set; }

    [JsonProperty("text")]
    public string Text { get; set; }

    [JsonProperty("attachments")]
    public List<Attachments> Attachments { get; set; }

    [JsonProperty("icon_emoji")]
    public string Icon
    {
        get { return ":computer:"; }
    }
}

#r "Newtonsoft.Json"

using Newtonsoft.Json;

public class Attachments
{
    [JsonProperty("color")]
    public string Colour { get; set; }

    [JsonProperty("title")]
    public string Title { get; set; }

    [JsonProperty("text")]
    public string Text { get; set; }
}
```
Remember to create these classes as .csx files for the Azure function to understand them.

#Step 4 - Create a slack client to post the message

Now that we have our message structure we can create a class to serialize and post the JSON to Slack using the Webhook created in **Step 1**, below is the code to do this,

```csharp
#r "Newtonsoft.Json"
#r "System.Web.Extensions"
#r "System.Web"

#load "SlackMessage.csx"
#load "Attachments.csx"

using System.Net;
using Newtonsoft.Json;
using System.Collections.Specialized;

public class SlackClient
{
    public static readonly string WebHook = @"https://hooks.slack.com/services/XXXXXXXX/XXXXXXXXXXXXXXXXXXXXXXXXXXX";

    public void SendMessage(SlackMessage message)
    {
        string payloadJson = JsonConvert.SerializeObject(message);
        
        using (WebClient client = new WebClient())
        {
            NameValueCollection data = new NameValueCollection();
            data["payload"] = payloadJson;
            client.UploadValues(WebHook, "POST", data);
        }
    }
}
```
Its good practice to move the Webhook URL into the settings file, for simplicity I have included it into this class.

#Step 5 - Queue Checker

Next we need to add code to loop through any connections string we pass it, check all the queues and send messages if we think there's something wrong.

```csharp
#r "Microsoft.WindowsAzure.Storage"

#load "SlackClient.csx"
#load "SlackMessage.csx"
#load "Attachments.csx"

using System.Collections.Generic;
using System.Linq;
using Microsoft.WindowsAzure.Storage;
using Microsoft.WindowsAzure.Storage.Auth;

public class PoisonQueueChecker
{
    public void CheckPoisonQueues(Dictionary<string, string> storageConnectionStrings)
    {
        var slackClient = new SlackClient();
        var slackMessage = new SlackMessage { Text = "Poison Queue Alerts", Channel = "poison-queue" };

        foreach (var storageConnectionString in storageConnectionStrings)
        {
            var storageCredentials = new StorageCredentials(storageConnectionString.Key, storageConnectionString.Value);
            var storageAccount = new CloudStorageAccount(storageCredentials, true);
            var queueClient = storageAccount.CreateCloudQueueClient();

            var queues = queueClient.ListQueues();
            foreach (var queue in queues)
            {
                queue.FetchAttributes();
                //Gets the total messages in the queue
                var queueCount = queue.ApproximateMessageCount;

                if (queueCount > 0)
                {
                    var isPoisonQueue = queue.Name.EndsWith("poison");
                    var attachment = new Attachments();
                    attachment.Title = $"Queue: {queue.Name}, Message Count: {queueCount}";
                    attachment.Colour = isPoisonQueue ? "danger" : "warning";

                    //Note the peek function will not dequeue the message
                    var message = queue.PeekMessage();
                    attachment.Text = $@"Insertion Time: {message.InsertionTime}, Sample Contents:\n" +
                                        $" {message.AsString}";                        

                    slackMessage.Attachments.Add(attachment);
                }
            }

            //Add a message showing all is well
            if (!slackMessage.Attachments.Any())
            {
                slackMessage.Attachments.Add(new Attachments { Title = "All queues are operational and empty", Colour = "good" });
            }
        }

        slackClient.SendMessage(slackMessage);
    }
}
```

#Step 6 - Being it all together

Final step is to hook up the functions run method like so:

```csharp
#load "PoisonQueueChecker.csx"

using System;
using System.Collections.Generic;

public static void Run(TimerInfo myTimer, TraceWriter log)
{
    log.Info($"C# Timer trigger function executed at: {DateTime.Now}");

    var storageConnectionStrings = new Dictionary();
    storageConnectionStrings.Add("storagename", "storagekey");

    var poisonQueueChecker = new PoisonQueueChecker();
    poisonQueueChecker.CheckPoisonQueues(storageConnectionStrings);
}
```

And that's it, 9 O'clock tomorrow you can finally start gaining visibility of those poison queues and start worrying about those dodgy lines of code causing your messages to be poisoned.

#Helpful Links

Custom Integrations

https://<<yourslackgroupname>>.slack.com/apps/manage/custom-integrations

Customising your slack message

https://api.slack.com/docs/messages/builder

How to send a slack message to your web hook:

https://api.slack.com/custom-integrations/incoming-webhooks

How to create a azure function:

https://docs.microsoft.com/en-us/azure/azure-functions/functions-create-first-azure-function

How to code up a azure function:

https://docs.microsoft.com/en-us/azure/azure-functions/functions-reference-csharp
1d:T1f26,Having looked at a number of projects in my lifetime, I always come across classes named something like "CustomerService" with similar variations (usually in the same project calling each other) ranging from "CustomerProvider / Helper /Manager / Store / etc...".

> There are two hard things in computer science: cache invalidation, naming things, and off-by-one errors - PhilKarlton

As a new developer working on a project it becomes really hard to figure out what the structure is, and in the case of adding code what to name classes. Naming and structure always seem to be a developers achilles heel(almost akin to superman and kryptonite).

So, I wanted to come up with a solution to the problem, something more structured that helps facilitate better naming and structuring and the way I decided to do this is through dependency injection. 

By now we are all familiar with Inversion of control, and comfortable using it to decouple dependencies within our applications. Most of us have dabbled with the usual suspects CastleWindsor, AutoFaq, Ninject to name a few. 

One thing people donâ€™t realise is we can also utilise these frameworks to enforce good structure as well as unit test the structure itself to ensure new developers donâ€™t stray from the named path. For the examples below I'm going to use Castle Windsor.

#Step 1 â€“ Define the structure and start naming the onion layers

The codebases of yester-year were usually architected using a n-tier structure typically following the pattern: 

Presentation Layer (Controller) > Business Layer (Service) > Data Layer (Repository)

As time progressed new patterns emerged the structure became more complex however we still try to adopt some form of onion layering within the application. Whether itâ€™s one onion or many within a single solution we should always strive to define what the layers are in the application.

So, to start we should define:

1. What the onion layer is named (forming groups of similar classes).
2. What the responsibility of each layer is.

#Step 2 â€“ Create your conventions

Now that we have some understanding of the layers, we can start defining them in code. I use an empty interface to do this. Note Castle distinguishes these layers as 'Conventions'.

```csharp
/// A class that contains business logic, it also does not directly access any data source.
public interface IService
{
}
```

Note the interface has no bearing on logic, and does not alter how the app behaves. Itâ€™s simply used as a marker to distinguish the layers of the application. A small description is also provided to define what the responsibility of the layer is. These conventions are also a way to document the structure of the application.

#Step 3 â€“ Install all dependencyâ€™s using the convention.

Now that we have a convention we can blanket install all classes subscribing to that convention, if your using castle Windsor there is a slight difference in how this is done depending on whether you apply the convention directly on the class itself or if you apply it to another interface.

**Applying it to an interface**

```csharp
/// A class that contains business logic, it also does not directly access any data source.
public interface IService
{
}

/// Blanket install all IServies
container.Register(Classes.FromAssembly(Assembly.Load("Assembly name goes here"))
    .BasedOn(IService)
    .WithService.AllInterfaces()
    .LifestyleSingleton());

/// Example usage
public interface ICustomerService : IService
public class CustomerService : ICustomerService
```

**Applying it to a class**

When applying it to class the installation has a slight difference.

```csharp
/// A class that contains a business rule, it validates whether the rule has been met
public interface IRule
{
 string ApplyRule();
}

container.Register(Classes.FromAssembly(Assembly.Load("Assembly name goes here"))
 .BasedOn(IRule)
 .WithService.Base()
 .LifestyleSingleton());

public class CustomerRule : IRule
```

When creating a new class that fits within a pre-defined convention installation becomes a walk in the park, just apply the convention interface and youâ€™re done.

#Step 4 â€“ Unit testing structure

Now that we have our convention setup and we are installing all classes with that convention we can apply a unit tests that will check against the structure. We are testing on two things here:

1. Only Services should have aâ€˜Serviceâ€™ Suffix
2. Only Services should exist in a â€˜Serviceâ€™ namespace

```csharp
[TestFixture]
public class TestSolutionConventionTests
{
    [SetUp]
    public void Setup()
    {
        // Register all dependencys in the project using castle
        RegisterDependencies(); 
    }

    [Test]
    public void OnlyServices_HaveServiceSuffix()
    {
        // Get access to IWindsorContainer
        var container = DependencyResolver.Container; 
        // Get all classes in the application where the name ends with Service (using reflection).
        var allServices = GetPublicClassesFromApplicationAssembly(c => c.Name.EndsWith("Service"), "Assembly name where service exists goes here");
        // Get all services installed within castles container that use the interface IService
        var registeredServices = GetImplementationTypesFor(typeof(IService), container);

        // Assert the names all match and are equal
        allServices.ToList().Should().Equal(registeredManagers, (ac, rc) => ac.Name == rc.Name);
    }

    [Test]
    public void OnlyServices_LiveInServicesNamespace()
    {
        var container = DependencyResolver.Container; 
        // Get all classes in the application where the namespace contains Service
        var allServices = GetPublicClassesFromApplicationAssembly(c => c.Namespace.Contains("Service"), "Assembly name where service exists goes here");
        var registeredServices = GetImplementationTypesFor(typeof(IService), container);

        allServices.ToList().Should().Equal(registeredManager, (ac, rc) => ac.Name == rc.Name);
    }

    private Type[] GetPublicClassesFromApplicationAssembly(Predicate where, string assemblyName)
    {
        return Assembly.Load(assemblyName).GetExportedTypes()
            .Where(t => t.IsClass)
            .Where(t => t.IsAbstract == false)
            .Where(where.Invoke)
            .OrderBy(t => t.Name)
            .ToArray();
    }

    private Type[] GetImplementationTypesFor(Type type, IWindsorContainer container)
    {
        return container.Kernel.GetAssignableHandlers(type)
            .Select(h => h.ComponentModel.Implementation)
            .OrderBy(t => t.Name)
            .ToArray();
    }
}
```

Picture below describes what these unit tests protect against:

![Convention Based Programming - Unit tests](//images.ctfassets.net/wjg1udsw901v/5DsNc00VzOuEgttxz02Wh0/a5d882981f31c1fad410cea2e30af5dd/conventionexample.jpg)

#Step 5 â€“ Introducing new conventions

As your solution evolves youâ€™re going to come across certain scenarios where the responsibilities of a class donâ€™t fit into the conventions defined (as we have a list of conventions with descriptions itâ€™s easy to distinguish if a new convention is needed). These scenarios will mainly occur at the beginning phase of a new application (as its rapidly evolving) and as conventions get defined you will find that having to define a new one will become an increasingly rare activity.

This process should mitigate the scenario of having a customer/service/manager/providerâ€¦

#Step 6 â€“ Sharing conventions across projects unified code base

Once weâ€™ve established some conventions for a project we can easily extract these out into a separate project and package it as a NuGet package. This allows us to apply the conventions to other solutions giving us a unified structure that looks the same from one solution to another.

New developers will surely appreciate this, and as a co-worker sitting next to them the wtf count will be below uncomfortable thresholds!1e:Tb11,This is a quick guide on how to split unit tests into different categories to decrease the time it takes for your CI build to run. The categories can be used to distinguish different areas of your tests to break down the CI Builds (typically used to run different categories in parallel) or to separate slow running tests into a separate build, all in the aim of speeding up the feedback cycle for developers. So to create a category you simple add a category attribute to either a test or a test fixture like so:

```csharp
[Category("CategoryOne")] 
[TestFixture] 
public void FunkyMethod() 
{ 
    string pointless = "this is code"; 
} 

[Category("CategoryFour")] 
[TestFixture] 
public class UpgradeControllerTests 
{ 
  ...
```

When segregating tests sometimes you will find a tests intersects multiple categories, in this case you can add multiple attributes. Later on we will see the different types of expressions you are able to enter when running the tests through TeamCity. Below is an example of using multiple categories

```csharp
[Category("CategoryOne")] 
[Category("CategoryTwo")] 
[Test] 
public void FunkyMethod() 
{ 
    string pointless = "this is code"; 
}
```

So far creating categories like this is fine however having magic strings all over your code is not great. So to fix this we can create a custom attribute which does exactly the same thing as shown below. The custom attribute inherits from CategoryAttribute.

```csharp
//Used for a test fixture 
[AttributeUsage(AttributeTargets.Class, AllowMultiple = false)] 
public sealed class CategoryFiveAttribute : CategoryAttribute { } 
//Used for a test 
[AttributeUsage(AttributeTargets.Method, AllowMultiple = false)] 
public sealed class CategoryThreeAttribute : CategoryAttribute { }
```

Now that we have the attributes ready we can use them like so.

```csharp
[TestFixture, CategoryFiveAttribute] 
public class SignOutControllerTests 
{ 
  ...
```

To configure team city to run certain categories is fairly straightforward. Start by creating a Build step with the runner type set to â€œNUnitâ€.  Under Run tests from select your test project dll file. And then under Nunit categories include list the categories you want to test out by writing

/Include: CategoryOne

Note that you can also do the inverse and exclude certain tests by adding the following in the section named Nunit categories exclude

/Exclude: CategoryOne

NUnit also supports quite complex expressions, to see a full list click here (section â€œSpecifying test categories to include or excludeâ€). 

A screenshot is included for a full list of settings.

![Nunit Test - Teamcity](//images.ctfassets.net/wjg1udsw901v/56EtPebPnDplF4gswxjpJH/f4ac2ffe01af7d3933838d4425d26bc8/teamcitytestcategory.png)

Once you have this in place your unit tests will run with lightening speed.7:[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"Home\",\"item\":\"https://www.faesel.com\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Blog\",\"item\":\"https://www.faesel.com/blog\"}]}"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"CollectionPage\",\"name\":\"Blog\",\"description\":\"Articles about technology, coding, and digital innovation\",\"url\":\"https://www.faesel.com/blog\"}"}}],["$","$L11",null,{"posts":[{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"5Y4yxKp7A7MgcystqRbOpl","type":"Entry","createdAt":"2023-03-17T14:03:54.327Z","updatedAt":"2023-03-17T16:06:56.451Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":22,"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"theCodeTransposerBlogPosts"}},"locale":"en-GB"},"fields":{"hero":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"4N5m97m5WfdJi5u7D8I4tX","type":"Asset","createdAt":"2023-03-17T11:24:11.827Z","updatedAt":"2023-03-17T11:24:11.827Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":5,"revision":1,"locale":"en-GB"},"fields":{"title":"electron-newrelic-opentelemetry","description":"","file":{"url":"//images.ctfassets.net/wjg1udsw901v/4N5m97m5WfdJi5u7D8I4tX/00d1a21ab7ce8db54af5fa6f84f89e9d/article-banner.jpg","details":{"size":25119,"image":{"width":1000,"height":500}},"fileName":"article-banner.jpg","contentType":"image/jpeg"}}},"title":"Electron & New Relic Integration Using Open Telemetry","tags":["electron","new-relic","open-telemetry","tracing","logging","jaeger","otlp","ipc-channel"],"slug":"electron-newrelic-integration-using-open-telemetry","datePublished":"2023-03-17T00:00+00:00","bodym":"$12"}},{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"5QTs3PIFwWHFqQzpcCUwHl","type":"Entry","createdAt":"2021-05-21T14:22:32.520Z","updatedAt":"2021-05-24T06:26:56.025Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":51,"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"theCodeTransposerBlogPosts"}},"locale":"en-GB"},"fields":{"hero":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"78Ws2s56LgCLoxkx3Xdcsl","type":"Asset","createdAt":"2021-05-21T14:22:25.484Z","updatedAt":"2021-05-21T14:22:25.484Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":4,"revision":1,"locale":"en-GB"},"fields":{"title":"Obsidian Logo","file":{"url":"//images.ctfassets.net/wjg1udsw901v/78Ws2s56LgCLoxkx3Xdcsl/083d00cd84eeec428087bbab65ae3580/obsidian-logo.png","details":{"size":31000,"image":{"width":1998,"height":666}},"fileName":"obsidian-logo.png","contentType":"image/png"}}},"title":"Why every developer needs to use Obsidian","tags":["obsidian","research","notes","markdown","ownership","mermaid","git","knowledge-graph","ide","plantuml","open-graph"],"slug":"why-every-developer-needs-to-use-obsidian","datePublished":"2021-05-24T00:00+01:00","bodym":"$13"}},{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"6mCFlKsDIry12uLc8ZDYNO","type":"Entry","createdAt":"2021-04-09T15:16:41.488Z","updatedAt":"2021-04-09T15:37:21.069Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":182,"revision":9,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"theCodeTransposerBlogPosts"}},"locale":"en-GB"},"fields":{"hero":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"4l5ONHEazPnD41lO0henyW","type":"Asset","createdAt":"2021-04-09T15:12:23.261Z","updatedAt":"2021-04-09T15:12:23.261Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":5,"revision":1,"locale":"en-GB"},"fields":{"title":"Deconstruct","file":{"url":"//images.ctfassets.net/wjg1udsw901v/4l5ONHEazPnD41lO0henyW/d4cbb6edf21c40cdb3e340faf620a270/deconstruction.jpg","details":{"size":80197,"image":{"width":1362,"height":686}},"fileName":"deconstruction.jpg","contentType":"image/jpeg"}}},"title":"How to Deconstruct objects in C# like we do in Javascript","tags":["c#","javascript","deconstruction","syntax",".net"],"slug":"deconstruct-objects-in-csharp-like-in-javascript","datePublished":"2021-04-09T00:00+01:00","bodym":"$14"}},{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"5pcAHCggLxz88Q0vkgjH2a","type":"Entry","createdAt":"2021-04-05T13:54:14.869Z","updatedAt":"2021-04-05T15:03:08.237Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":1184,"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"theCodeTransposerBlogPosts"}},"locale":"en-GB"},"fields":{"hero":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"74JrRnpexhOnSAsBwNOPV7","type":"Asset","createdAt":"2021-04-05T13:33:18.027Z","updatedAt":"2021-04-05T13:33:18.027Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":5,"revision":1,"locale":"en-GB"},"fields":{"title":"GraphQL With Hot Chocolate","file":{"url":"//images.ctfassets.net/wjg1udsw901v/74JrRnpexhOnSAsBwNOPV7/635bc389cea0de36f3158df45483ae85/graphql.jpg","details":{"size":42436,"image":{"width":1120,"height":630}},"fileName":"graphql.jpg","contentType":"image/jpeg"}}},"title":"ASP.NET GraphQL server with Hot Chocolate","tags":["graphql","hotchocolate","graphql-voyager","asp.net","authentication","authorization","versioning","rest","chilli-cream","logging","open-telemetry","banana-cake-pop"],"slug":"aspnet-graphql-server-with-hot-chocolate","datePublished":"2021-04-05T00:00+01:00","bodym":"$15"}},{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"5dxjjWwRr6271r9804OQuh","type":"Entry","createdAt":"2021-01-19T08:52:14.211Z","updatedAt":"2021-01-25T10:46:49.931Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":912,"revision":5,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"theCodeTransposerBlogPosts"}},"locale":"en-GB"},"fields":{"hero":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"1IeybLQIjDnbaXTl4sbqTn","type":"Asset","createdAt":"2021-01-18T17:25:12.985Z","updatedAt":"2021-01-18T17:25:12.985Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":5,"revision":1,"locale":"en-GB"},"fields":{"title":".env + dotnet core","file":{"url":"//images.ctfassets.net/wjg1udsw901v/1IeybLQIjDnbaXTl4sbqTn/294b0ef4ad9095ee3633f6c38a0e35aa/hero.png","details":{"size":55286,"image":{"width":1280,"height":720}},"fileName":"hero.png","contentType":"image/png"}}},"title":"Adding environments to ASP.NET Core with React.js SPA","tags":["react","spa","asp.net","dotnet core","environments","env-cmd","shx","template","msbuild",".env"],"slug":"aspnet-core-react-spa-adding-environments","datePublished":"2021-01-19T00:00+00:00","bodym":"$16"}},{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"6Tifdl2jmnr4p5XGBzYzt2","type":"Entry","createdAt":"2020-12-18T17:20:58.567Z","updatedAt":"2021-01-25T11:02:41.835Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":972,"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"theCodeTransposerBlogPosts"}},"locale":"en-GB"},"fields":{"hero":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"5nkhsfNMJDJ5NGcAfvQ2Lt","type":"Asset","createdAt":"2020-12-18T17:17:28.507Z","updatedAt":"2020-12-18T17:17:28.507Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":5,"revision":1,"locale":"en-GB"},"fields":{"title":"Az Lazy","file":{"url":"//images.ctfassets.net/wjg1udsw901v/5nkhsfNMJDJ5NGcAfvQ2Lt/a2ac18006da7a4865044b77365b55987/AzLazy.png","details":{"size":103712,"image":{"width":2304,"height":1296}},"fileName":"AzLazy.png","contentType":"image/png"}}},"title":"My journey of creating a .NET CLI tool","tags":["cli","azure","queues","table-storage","containers","blob","azure-storage","dotnet-tools","az-lazy","console","commandline","dotnet-tools"],"slug":"my-journey-of-creating-a-dotnet-cli-tool","datePublished":"2020-12-18T00:00+00:00","bodym":"$17"}},{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"3llNnDbl34NvCQHa51SjWT","type":"Entry","createdAt":"2020-09-08T12:51:10.019Z","updatedAt":"2020-09-09T11:28:40.087Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":1110,"revision":14,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"theCodeTransposerBlogPosts"}},"locale":"en-GB"},"fields":{"hero":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"5m1MwxccFfmDxkLKcq3dBt","type":"Asset","createdAt":"2020-09-08T12:50:44.778Z","updatedAt":"2020-09-08T12:50:44.778Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":6,"revision":1,"locale":"en-GB"},"fields":{"title":"GRPC Logo","file":{"url":"//images.ctfassets.net/wjg1udsw901v/5m1MwxccFfmDxkLKcq3dBt/b54fc31b09d0a266a3d8cd5082839976/grpc-logojpg.jpg","details":{"size":80653,"image":{"width":1600,"height":768}},"fileName":"grpc-logojpg.jpg","contentType":"image/jpeg"}}},"title":".NET & GRPC What they forgot to tell you","tags":["grpc",".net","c#","asp.net","grpc-web","rest","nswag","proto-files","nuget","grpc-reflection","bloomrpc"],"slug":"dotnet-grpc-forgot-to-tell-you","datePublished":"2020-09-08T00:00+01:00","bodym":"$18"}},{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"3Wyw7CR6bTwM5Bl8B5mqL8","type":"Entry","createdAt":"2020-07-19T11:07:03.892Z","updatedAt":"2020-07-25T14:11:42.345Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":633,"revision":8,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"theCodeTransposerBlogPosts"}},"locale":"en-GB"},"fields":{"hero":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"5WxA9oRmhEgKswEWyFjdbM","type":"Asset","createdAt":"2020-07-24T17:37:02.272Z","updatedAt":"2020-07-25T09:49:11.408Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":9,"revision":2,"locale":"en-GB"},"fields":{"title":"Windows Terminal Icon","file":{"url":"//images.ctfassets.net/wjg1udsw901v/5WxA9oRmhEgKswEWyFjdbM/e0efe86ff8b93567fac16c1cfb7d951f/windowsterminalicon.jpg","details":{"size":42772,"image":{"width":1365,"height":768}},"fileName":"windowsterminalicon.jpg","contentType":"image/jpeg"}}},"title":"Evolving your Windows Terminal using Powershell libraries","tags":["powershell","wsl","windows-terminal","powershell-gallery"],"slug":"evolving-windows-terminal","datePublished":"2020-07-25T00:00+01:00","bodym":"$19"}},{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"6QEFdX8pPUEjsy1FkjARIe","type":"Entry","createdAt":"2020-07-12T11:28:51.943Z","updatedAt":"2020-07-12T13:15:52.717Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":633,"revision":4,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"theCodeTransposerBlogPosts"}},"locale":"en-GB"},"fields":{"hero":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"6BwwZovIUzXkE7j7ZeVxM6","type":"Asset","createdAt":"2020-07-12T11:27:18.016Z","updatedAt":"2020-07-12T11:27:18.016Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":4,"revision":1,"locale":"en-GB"},"fields":{"title":"Helmet","file":{"url":"//images.ctfassets.net/wjg1udsw901v/6BwwZovIUzXkE7j7ZeVxM6/f7eed6871e869df95a84ef57d8df7ed6/gladiator-1931077_1280.jpg","details":{"size":208632,"image":{"width":1280,"height":853}},"fileName":"gladiator-1931077_1280.jpg","contentType":"image/jpeg"}}},"title":"GatsbyJS SEO and Open Graph with Helmet","tags":["helmet","twitter","seo","linked-data","gatsbyjs","json-ld","open-graph"],"slug":"gatsby-seo-opengraph-helmet","datePublished":"2020-07-12T00:00+01:00","bodym":"$1a"}},{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"aLxVZHYGnw53GvdtjMsio","type":"Entry","createdAt":"2020-07-08T07:23:37.130Z","updatedAt":"2021-01-25T10:45:58.773Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":412,"revision":7,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"theCodeTransposerBlogPosts"}},"locale":"en-GB"},"fields":{"hero":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"6hjsGXkoyitmyiEuBdeTP2","type":"Asset","createdAt":"2020-07-08T07:28:31.589Z","updatedAt":"2020-07-08T07:28:31.589Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":4,"revision":1,"locale":"en-GB"},"fields":{"title":"Gatsby JS","file":{"url":"//images.ctfassets.net/wjg1udsw901v/6hjsGXkoyitmyiEuBdeTP2/c77e74af9235ac775f18836e2de07cac/gatsby-logo.jpg","details":{"size":24454,"image":{"width":960,"height":540}},"fileName":"gatsby-logo.jpg","contentType":"image/jpeg"}}},"title":"Creating my dream tech blog with GatsbyJS","tags":["contentful","disqus","google-analytics","blog","react","graphql"],"slug":"gatsby-tech-blog-starter","datePublished":"2020-07-08T00:00+01:00","bodym":"$1b"}},{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"7HTIP6hGAwuoB7DHLmEhUi","type":"Entry","createdAt":"2020-06-28T17:31:51.294Z","updatedAt":"2020-07-02T17:43:34.259Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":72,"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"theCodeTransposerBlogPosts"}},"locale":"en-GB"},"fields":{"hero":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"UMa2shO53yjhwxv5PF0go","type":"Asset","createdAt":"2020-07-02T17:43:16.514Z","updatedAt":"2020-07-02T17:43:16.514Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":6,"revision":1,"locale":"en-GB"},"fields":{"title":"Azure Poison Queues Monitoring","file":{"url":"//images.ctfassets.net/wjg1udsw901v/UMa2shO53yjhwxv5PF0go/cbf2e4489e801053a91d77a038dcbde9/tobias-tullius-4dKy7d3lkKM-unsplash.jpg","details":{"size":4379377,"image":{"width":4912,"height":3264}},"fileName":"tobias-tullius-4dKy7d3lkKM-unsplash.jpg","contentType":"image/jpeg"}}},"title":"Making a Azure poison queue Slack notifier","tags":["azure","poison queue","monitoring","slack","azure-queues"],"slug":"azure-poison-queue-notifier","datePublished":"2017-09-23T00:00+01:00","bodym":"$1c"}},{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"7L2t1iKJrp2lW1A5OPQqqC","type":"Entry","createdAt":"2020-06-28T18:24:06.856Z","updatedAt":"2020-07-02T17:50:26.494Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":59,"revision":2,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"theCodeTransposerBlogPosts"}},"locale":"en-GB"},"fields":{"hero":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"27s4Gn7WXRTKhSaaWBU2RN","type":"Asset","createdAt":"2020-07-02T17:49:41.191Z","updatedAt":"2020-07-02T17:49:41.191Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":5,"revision":1,"locale":"en-GB"},"fields":{"title":"Contention Based Programming","file":{"url":"//images.ctfassets.net/wjg1udsw901v/27s4Gn7WXRTKhSaaWBU2RN/c057b731733a4d8943365bdeaeb71147/alain-pham-P_qvsF7Yodw-unsplash.jpg","details":{"size":4213457,"image":{"width":6016,"height":4016}},"fileName":"alain-pham-P_qvsF7Yodw-unsplash.jpg","contentType":"image/jpeg"}}},"title":"Fix poor project structure with Convention Based Programming","tags":["convention","unit-test","project-structure"],"slug":"convention-based-programming","datePublished":"2017-08-20T00:00+01:00","bodym":"$1d"}},{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"3JPof9nR7LY1oL1562Rj65","type":"Entry","createdAt":"2020-06-28T18:42:22.087Z","updatedAt":"2021-01-25T10:45:11.461Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":31,"revision":3,"contentType":{"sys":{"type":"Link","linkType":"ContentType","id":"theCodeTransposerBlogPosts"}},"locale":"en-GB"},"fields":{"hero":{"metadata":{"tags":[],"concepts":[]},"sys":{"space":{"sys":{"type":"Link","linkType":"Space","id":"wjg1udsw901v"}},"id":"3YSq2wLiYV0f3KvoXUXjXL","type":"Asset","createdAt":"2020-07-02T17:47:11.331Z","updatedAt":"2020-07-02T17:47:11.331Z","environment":{"sys":{"id":"master","type":"Link","linkType":"Environment"}},"publishedVersion":5,"revision":1,"locale":"en-GB"},"fields":{"title":"Unit Test Traffic Light","file":{"url":"//images.ctfassets.net/wjg1udsw901v/3YSq2wLiYV0f3KvoXUXjXL/19aa4d78b6d63287928c6d40f2e36d99/harshal-desai-0hCIrw8dVfE-unsplash.jpg","details":{"size":370069,"image":{"width":3872,"height":2592}},"fileName":"harshal-desai-0hCIrw8dVfE-unsplash.jpg","contentType":"image/jpeg"}}},"title":"Splitting NUnit Unit Tests With TeamCity To Decrease CI Time","tags":["nunit","unit-tests","continuous-integration","ci","teamcity"],"slug":"nunit-test-ci-split","datePublished":"2017-04-01T00:00+01:00","bodym":"$1e"}}],"allTags":[".env",".net","asp.net","authentication","authorization","az-lazy","azure","azure-queues","azure-storage","banana-cake-pop","blob","blog","bloomrpc","c#","chilli-cream","ci","cli","commandline","console","containers","contentful","continuous-integration","convention","deconstruction","disqus","dotnet core","dotnet-tools","electron","env-cmd","environments","gatsbyjs","git","google-analytics","graphql","graphql-voyager","grpc","grpc-reflection","grpc-web","helmet","hotchocolate","ide","ipc-channel","jaeger","javascript","json-ld","knowledge-graph","linked-data","logging","markdown","mermaid","monitoring","msbuild","new-relic","notes","nswag","nuget","nunit","obsidian","open-graph","open-telemetry","otlp","ownership","plantuml","poison queue","powershell","powershell-gallery","project-structure","proto-files","queues","react","research","rest","seo","shx","slack","spa","syntax","table-storage","teamcity","template","tracing","twitter","unit-test","unit-tests","versioning","windows-terminal","wsl"]}]]
